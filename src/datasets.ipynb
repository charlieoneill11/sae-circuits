{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greater-than task...\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 16.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greater-than task done for gpt2-small.\n",
      "\n",
      "\n",
      "Loaded pretrained model gpt2-medium into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:03<00:00,  5.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greater-than task done for gpt2-medium.\n",
      "\n",
      "\n",
      "Loaded pretrained model gpt2-large into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:05<00:00,  3.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greater-than task done for gpt2-large.\n",
      "\n",
      "\n",
      "Loaded pretrained model gpt2-xl into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:08<00:00,  2.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greater-than task done for gpt2-xl.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "from logging import warning\n",
    "from typing import Union, List\n",
    "from site import PREFIXES\n",
    "import warnings\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "import random\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rd\n",
    "import copy\n",
    "import random\n",
    "from typing import List, Union\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "### GREATER-THAN TASK ###\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "print(\"Greater-than task...\")\n",
    "\n",
    "models = ['gpt2-small', 'gpt2-medium', 'gpt2-large', 'gpt2-xl']\n",
    "\n",
    "for model_name in models:\n",
    "    model = HookedTransformer.from_pretrained(model_name, device=device)\n",
    "\n",
    "    # Turn grad off\n",
    "    torch.set_grad_enabled(False)\n",
    "\n",
    "    def generate_real_sentence(noun: str, year: int, eos: bool = False) -> str:\n",
    "        century = year // 100\n",
    "        sentence = f\"The {noun} lasted from the year {year} to the year {century}\"\n",
    "        if eos:\n",
    "            sentence = \" \" + sentence\n",
    "        return sentence\n",
    "\n",
    "    def real_sentence_prompt(eos: bool = False) -> List[str]:\n",
    "        sentence = f\"The NOUN lasted from the year XX1 YY to the year XX2\".split()\n",
    "        if eos:\n",
    "            sentence = [\"\"] + sentence\n",
    "        return sentence\n",
    "\n",
    "    def generate_bad_sentence(noun: str, year: int, eos: bool = False) -> str:\n",
    "        century = year // 100\n",
    "        #sentence = f\"The {noun} lasted from the year {century}01 to the year {century-1}\"\n",
    "        sentence = f\"The {noun} lasted from the year {year} to the year {century-1}\"\n",
    "        if eos:\n",
    "            sentence = \" \" + sentence\n",
    "        return sentence\n",
    "\n",
    "    def bad_sentence_prompt(eos: bool = False) -> List[str]:\n",
    "        sentence = f\"The NOUN lasted from the year XX1 01 to the year XX2\".split()\n",
    "        if eos:\n",
    "            sentence = [\"\"] + sentence\n",
    "        return sentence\n",
    "\n",
    "    def is_valid_year(year: str, model) -> bool:\n",
    "        _year = \" \" + year\n",
    "        token = model.to_tokens(_year)\n",
    "        detok = model.to_string(token)\n",
    "        return len(detok) == 2 and len(detok[1]) == 2\n",
    "\n",
    "    class YearDataset:\n",
    "        years_to_sample_from: torch.Tensor\n",
    "        N: int\n",
    "        ordered: bool\n",
    "        eos: bool\n",
    "\n",
    "        nouns: List[str]\n",
    "        years: torch.Tensor\n",
    "        years_YY: torch.Tensor\n",
    "        good_sentences: List[str]\n",
    "        bad_sentences: List[str]\n",
    "        good_toks: torch.Tensor\n",
    "        bad_toks: torch.Tensor\n",
    "        good_prompt: List[str]\n",
    "        bad_prompt: List[str]\n",
    "        good_mask: torch.Tensor\n",
    "        model: HookedTransformer\n",
    "\n",
    "        def __init__(\n",
    "            self,\n",
    "            years_to_sample_from,\n",
    "            N: int,\n",
    "            nouns: Union[str, List[str], Path],\n",
    "            model: HookedTransformer,\n",
    "            balanced: bool = True,\n",
    "            eos: bool = False,\n",
    "            device: str = \"cpu\",\n",
    "        ):\n",
    "            self.years_to_sample_from = years_to_sample_from\n",
    "            self.N = N\n",
    "            self.eos = eos\n",
    "            self.model = model\n",
    "\n",
    "            if isinstance(nouns, str):\n",
    "                noun_list = [nouns]\n",
    "            elif isinstance(nouns, list):\n",
    "                noun_list = nouns\n",
    "            elif isinstance(nouns, Path):\n",
    "                with open(nouns, \"r\") as f:\n",
    "                    noun_list = [line.strip() for line in f]\n",
    "            else:\n",
    "                raise ValueError(f\"Got bad type of nouns: {type(nouns)}; for nouns: {nouns}\")\n",
    "\n",
    "            self.nouns = random.choices(noun_list, k=N)\n",
    "\n",
    "            if balanced:\n",
    "                years = []\n",
    "                current_year = 2\n",
    "                years_to_sample_from_YY = self.years_to_sample_from % 100\n",
    "                for i in range(N):\n",
    "                    sample_pool = self.years_to_sample_from[years_to_sample_from_YY == current_year]\n",
    "                    years.append(sample_pool[random.randrange(len(sample_pool))])\n",
    "                    current_year += 1\n",
    "                    if current_year >= 99:\n",
    "                        current_year -= 97\n",
    "                self.years = torch.tensor(years)\n",
    "            else:\n",
    "                self.years = torch.tensor(self.years_to_sample_from[torch.randint(0, len(self.years_to_sample_from), (N,))])\n",
    "\n",
    "            self.years_XX = self.years // 100\n",
    "            self.years_YY = self.years % 100\n",
    "\n",
    "            self.good_sentences = [\n",
    "                generate_real_sentence(noun, int(year.item()), eos=eos) for noun, year in zip(self.nouns, self.years)\n",
    "            ]\n",
    "            self.bad_sentences = [\n",
    "                generate_bad_sentence(noun, int(year.item()), eos=eos) for noun, year in zip(self.nouns, self.years)\n",
    "            ]\n",
    "\n",
    "            self.good_prompt = real_sentence_prompt(eos=eos)\n",
    "            self.bad_prompt = bad_sentence_prompt(eos=eos)\n",
    "\n",
    "        def __len__(self):\n",
    "            return self.N\n",
    "\n",
    "    # # Instantiate the model\n",
    "    # model = HookedTransformer.from_pretrained(\"gpt2-small\", device='cpu')\n",
    "\n",
    "    # Define your nouns and years\n",
    "    nouns = [\n",
    "        \"abduction\", \"accord\", \"affair\", \"agreement\", \"appraisal\",\n",
    "        \"assaults\", \"assessment\", \"attack\", \"attempts\", \"campaign\", \n",
    "        \"captivity\", \"case\", \"challenge\", \"chaos\", \"clash\", \n",
    "        \"collaboration\", \"coma\", \"competition\", \"confrontation\", \"consequence\", \n",
    "        \"conspiracy\", \"construction\", \"consultation\", \"contact\",\n",
    "        \"contract\", \"convention\", \"cooperation\", \"custody\", \"deal\", \n",
    "        \"decline\", \"decrease\", \"demonstrations\", \"development\", \"disagreement\", \n",
    "        \"disorder\", \"dispute\", \"domination\", \"dynasty\", \"effect\", \n",
    "        \"effort\", \"employment\", \"endeavor\", \"engagement\",\n",
    "        \"epidemic\", \"evaluation\", \"exchange\", \"existence\", \"expansion\", \n",
    "        \"expedition\", \"experiments\", \"fall\", \"fame\", \"flights\",\n",
    "        \"friendship\", \"growth\", \"hardship\", \"hostility\", \"illness\", \n",
    "        \"impact\", \"imprisonment\", \"improvement\", \"incarceration\",\n",
    "        \"increase\", \"insurgency\", \"invasion\", \"investigation\", \"journey\", \n",
    "    ]  # Example nouns list\n",
    "    years_to_sample_from = torch.arange(1200, 2000)  # Example years range\n",
    "\n",
    "    # Instantiate the YearDataset class\n",
    "    dataset = YearDataset(\n",
    "        years_to_sample_from=years_to_sample_from,\n",
    "        N=10,  # Number of samples you want\n",
    "        nouns=nouns,\n",
    "        model=model,\n",
    "        balanced=True,  # Whether to balance the years in the dataset\n",
    "        eos=False,  # Whether to add an end-of-sentence token\n",
    "        device=\"cpu\"  # Device to use ('cpu' or 'cuda')\n",
    "    )\n",
    "\n",
    "    # Set up the model\n",
    "    def prompt_to_resid_stream(prompt: str, model: HookedTransformer, resid_type: str = 'accumulated', position: str = 'last') -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Convert a prompt to a residual stream of size (n_layers, d_model)\n",
    "        \"\"\"\n",
    "        # Run the model over the prompt\n",
    "        with torch.no_grad():\n",
    "            _, cache = model.run_with_cache(prompt)\n",
    "\n",
    "            # Get the accumulated residuals\n",
    "            if resid_type == 'accumulated':\n",
    "                resid, _ = cache.accumulated_resid(return_labels=True, apply_ln=True)\n",
    "            elif resid_type == 'decomposed':\n",
    "                resid, _ = cache.decompose_resid(return_labels=True)\n",
    "            elif resid_type == 'heads':\n",
    "                cache.compute_head_results()\n",
    "                head_resid, head_labels = cache.stack_head_results(return_labels=True)\n",
    "                #mlp_resid, mlp_labels = cache.decompose_resid(mode='mlp', incl_embeds=False, return_labels=True)\n",
    "                # Combine\n",
    "                # resid = torch.cat([head_resid, mlp_resid], dim=0)\n",
    "                # labels = head_labels + mlp_labels\n",
    "                resid = head_resid\n",
    "                labels = head_labels\n",
    "            else:\n",
    "                raise ValueError(\"resid_type must be one of 'accumulated', 'decomposed', 'heads'\")\n",
    "\n",
    "        # POSITION\n",
    "        if position == 'last':\n",
    "            last_token_accum = resid[:, 0, -1, :]  # layer, batch, pos, d_model\n",
    "        elif position == 'mean':\n",
    "            last_token_accum = resid.mean(dim=2).squeeze()\n",
    "        else:\n",
    "            raise ValueError(\"position must be one of 'last', 'mean'\")\n",
    "        return last_token_accum, labels\n",
    "\n",
    "\n",
    "    def all_prompts_to_resid_streams_gt(prompts, prompts_cf, model, resid_type='accumulated', position='mean'):\n",
    "        \"\"\"\n",
    "        Convert all prompts and counterfactual prompts to residual streams\n",
    "        \"\"\"\n",
    "        # Stack prompts and prompts cf\n",
    "        resid_streams = []\n",
    "        # Combine the lists of strs\n",
    "        all_prompts = prompts + prompts_cf\n",
    "        for i in tqdm(range(len(all_prompts))):\n",
    "            prompt = all_prompts[i]\n",
    "            resid_stream, labels = prompt_to_resid_stream(prompt, model, resid_type, position)\n",
    "            resid_streams.append(resid_stream)\n",
    "        # Stack the residual streams into a single tensor\n",
    "        return torch.stack(resid_streams), labels\n",
    "\n",
    "    prompts = dataset.good_sentences\n",
    "    prompts_cf = dataset.bad_sentences\n",
    "\n",
    "    resid_streams, labels = all_prompts_to_resid_streams_gt(prompts, prompts_cf, model, resid_type='heads', position='mean')\n",
    "\n",
    "    ground_truth = [\n",
    "        (0, 3), (0, 5), (0, 1), (5, 5), (6, 1), (6, 9), (7, 10), (8, 11), (9, 1), # attention heads\n",
    "    ]\n",
    "\n",
    "    # Save to ..data/gt folder\n",
    "    torch.save(resid_streams, f\"../data/gt/model_sizes/resid_heads_mean_{model_name}.pt\")\n",
    "    all_prompts = prompts + prompts_cf\n",
    "    torch.save(all_prompts, f\"../data/gt/model_sizes/prompts_heads_mean_{model_name}.pt\")\n",
    "    torch.save(labels, f\"../data/gt/model_sizes/labels_heads_mean_{model_name}.pt\")\n",
    "    torch.save(ground_truth, \"../data/gt/model_sizes/ground_truth.pt\")\n",
    "\n",
    "    # Flush torch cache\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"Greater-than task done for {model_name}.\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greater-than task...\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:23<00:00, 21.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greater-than task done.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "print(\"Greater-than task...\")\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "\n",
    "def generate_real_sentence(noun: str, year: int, eos: bool = False) -> str:\n",
    "    century = year // 100\n",
    "    sentence = f\"The {noun} lasted from the year {year} to the year {century}\"\n",
    "    if eos:\n",
    "        sentence = \" \" + sentence\n",
    "    return sentence\n",
    "\n",
    "def real_sentence_prompt(eos: bool = False) -> List[str]:\n",
    "    sentence = f\"The NOUN lasted from the year XX1 YY to the year XX2\".split()\n",
    "    if eos:\n",
    "        sentence = [\"\"] + sentence\n",
    "    return sentence\n",
    "\n",
    "def generate_bad_sentence(noun: str, year: int, eos: bool = False) -> str:\n",
    "    century = year // 100\n",
    "    sentence = f\"The {noun} lasted from the year {century}01 to the year {century}\"\n",
    "    #sentence = f\"The {noun} lasted from the year {year} to the year {century-1}\"\n",
    "    if eos:\n",
    "        sentence = \" \" + sentence\n",
    "    return sentence\n",
    "\n",
    "def bad_sentence_prompt(eos: bool = False) -> List[str]:\n",
    "    sentence = f\"The NOUN lasted from the year XX1 01 to the year XX2\".split()\n",
    "    if eos:\n",
    "        sentence = [\"\"] + sentence\n",
    "    return sentence\n",
    "\n",
    "def is_valid_year(year: str, model) -> bool:\n",
    "    _year = \" \" + year\n",
    "    token = model.to_tokens(_year)\n",
    "    detok = model.to_string(token)\n",
    "    return len(detok) == 2 and len(detok[1]) == 2\n",
    "\n",
    "class YearDataset:\n",
    "    years_to_sample_from: torch.Tensor\n",
    "    N: int\n",
    "    ordered: bool\n",
    "    eos: bool\n",
    "\n",
    "    nouns: List[str]\n",
    "    years: torch.Tensor\n",
    "    years_YY: torch.Tensor\n",
    "    good_sentences: List[str]\n",
    "    bad_sentences: List[str]\n",
    "    good_toks: torch.Tensor\n",
    "    bad_toks: torch.Tensor\n",
    "    good_prompt: List[str]\n",
    "    bad_prompt: List[str]\n",
    "    good_mask: torch.Tensor\n",
    "    model: HookedTransformer\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        years_to_sample_from,\n",
    "        N: int,\n",
    "        nouns: Union[str, List[str], Path],\n",
    "        model: HookedTransformer,\n",
    "        balanced: bool = True,\n",
    "        eos: bool = False,\n",
    "        device: str = \"cpu\",\n",
    "    ):\n",
    "        self.years_to_sample_from = years_to_sample_from\n",
    "        self.N = N\n",
    "        self.eos = eos\n",
    "        self.model = model\n",
    "\n",
    "        if isinstance(nouns, str):\n",
    "            noun_list = [nouns]\n",
    "        elif isinstance(nouns, list):\n",
    "            noun_list = nouns\n",
    "        elif isinstance(nouns, Path):\n",
    "            with open(nouns, \"r\") as f:\n",
    "                noun_list = [line.strip() for line in f]\n",
    "        else:\n",
    "            raise ValueError(f\"Got bad type of nouns: {type(nouns)}; for nouns: {nouns}\")\n",
    "\n",
    "        self.nouns = random.choices(noun_list, k=N)\n",
    "\n",
    "        if balanced:\n",
    "            years = []\n",
    "            current_year = 2\n",
    "            years_to_sample_from_YY = self.years_to_sample_from % 100\n",
    "            for i in range(N):\n",
    "                sample_pool = self.years_to_sample_from[years_to_sample_from_YY == current_year]\n",
    "                years.append(sample_pool[random.randrange(len(sample_pool))])\n",
    "                current_year += 1\n",
    "                if current_year >= 99:\n",
    "                    current_year -= 97\n",
    "            self.years = torch.tensor(years)\n",
    "        else:\n",
    "            self.years = torch.tensor(self.years_to_sample_from[torch.randint(0, len(self.years_to_sample_from), (N,))])\n",
    "\n",
    "        self.years_XX = self.years // 100\n",
    "        self.years_YY = self.years % 100\n",
    "\n",
    "        self.good_sentences = [\n",
    "            generate_real_sentence(noun, int(year.item()), eos=eos) for noun, year in zip(self.nouns, self.years)\n",
    "        ]\n",
    "        self.bad_sentences = [\n",
    "            generate_bad_sentence(noun, int(year.item()), eos=eos) for noun, year in zip(self.nouns, self.years)\n",
    "        ]\n",
    "\n",
    "        self.good_prompt = real_sentence_prompt(eos=eos)\n",
    "        self.bad_prompt = bad_sentence_prompt(eos=eos)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.N\n",
    "\n",
    "# Instantiate the model\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\", device='cpu')\n",
    "\n",
    "# Define your nouns and years\n",
    "nouns = [\n",
    "    \"abduction\", \"accord\", \"affair\", \"agreement\", \"appraisal\",\n",
    "    \"assaults\", \"assessment\", \"attack\", \"attempts\", \"campaign\", \n",
    "    \"captivity\", \"case\", \"challenge\", \"chaos\", \"clash\", \n",
    "    \"collaboration\", \"coma\", \"competition\", \"confrontation\", \"consequence\", \n",
    "    \"conspiracy\", \"construction\", \"consultation\", \"contact\",\n",
    "    \"contract\", \"convention\", \"cooperation\", \"custody\", \"deal\", \n",
    "    \"decline\", \"decrease\", \"demonstrations\", \"development\", \"disagreement\", \n",
    "    \"disorder\", \"dispute\", \"domination\", \"dynasty\", \"effect\", \n",
    "    \"effort\", \"employment\", \"endeavor\", \"engagement\",\n",
    "    \"epidemic\", \"evaluation\", \"exchange\", \"existence\", \"expansion\", \n",
    "    \"expedition\", \"experiments\", \"fall\", \"fame\", \"flights\",\n",
    "    \"friendship\", \"growth\", \"hardship\", \"hostility\", \"illness\", \n",
    "    \"impact\", \"imprisonment\", \"improvement\", \"incarceration\",\n",
    "    \"increase\", \"insurgency\", \"invasion\", \"investigation\", \"journey\", \n",
    "]  # Example nouns list\n",
    "years_to_sample_from = torch.arange(1200, 2000)  # Example years range\n",
    "\n",
    "# Instantiate the YearDataset class\n",
    "dataset = YearDataset(\n",
    "    years_to_sample_from=years_to_sample_from,\n",
    "    N=250,  # Number of samples you want\n",
    "    nouns=nouns,\n",
    "    model=model,\n",
    "    balanced=True,  # Whether to balance the years in the dataset\n",
    "    eos=False,  # Whether to add an end-of-sentence token\n",
    "    device=\"cpu\"  # Device to use ('cpu' or 'cuda')\n",
    ")\n",
    "\n",
    "# Set up the model\n",
    "def prompt_to_resid_stream(prompt: str, model: HookedTransformer, resid_type: str = 'accumulated', position: str = 'last') -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convert a prompt to a residual stream of size (n_layers, d_model)\n",
    "    \"\"\"\n",
    "    # Run the model over the prompt\n",
    "    with torch.no_grad():\n",
    "        _, cache = model.run_with_cache(prompt)\n",
    "\n",
    "        # Get the accumulated residuals\n",
    "        if resid_type == 'accumulated':\n",
    "            resid, _ = cache.accumulated_resid(return_labels=True, apply_ln=True)\n",
    "        elif resid_type == 'decomposed':\n",
    "            resid, _ = cache.decompose_resid(return_labels=True)\n",
    "        elif resid_type == 'heads':\n",
    "            cache.compute_head_results()\n",
    "            head_resid, head_labels = cache.stack_head_results(return_labels=True)\n",
    "            #mlp_resid, mlp_labels = cache.decompose_resid(mode='mlp', incl_embeds=False, return_labels=True)\n",
    "            # Combine\n",
    "            # resid = torch.cat([head_resid, mlp_resid], dim=0)\n",
    "            # labels = head_labels + mlp_labels\n",
    "            resid = head_resid\n",
    "            labels = head_labels\n",
    "        else:\n",
    "            raise ValueError(\"resid_type must be one of 'accumulated', 'decomposed', 'heads'\")\n",
    "\n",
    "    # POSITION\n",
    "    if position == 'last':\n",
    "        last_token_accum = resid[:, 0, -1, :]  # layer, batch, pos, d_model\n",
    "    elif position == 'mean':\n",
    "        last_token_accum = resid.mean(dim=2).squeeze()\n",
    "    else:\n",
    "        raise ValueError(\"position must be one of 'last', 'mean'\")\n",
    "    return last_token_accum, labels\n",
    "\n",
    "\n",
    "def all_prompts_to_resid_streams_gt(prompts, prompts_cf, model, resid_type='accumulated', position='mean'):\n",
    "    \"\"\"\n",
    "    Convert all prompts and counterfactual prompts to residual streams\n",
    "    \"\"\"\n",
    "    # Stack prompts and prompts cf\n",
    "    resid_streams = []\n",
    "    # Combine the lists of strs\n",
    "    all_prompts = prompts + prompts_cf\n",
    "    for i in tqdm(range(len(all_prompts))):\n",
    "        prompt = all_prompts[i]\n",
    "        resid_stream, labels = prompt_to_resid_stream(prompt, model, resid_type, position)\n",
    "        resid_streams.append(resid_stream)\n",
    "    # Stack the residual streams into a single tensor\n",
    "    return torch.stack(resid_streams), labels\n",
    "\n",
    "prompts = dataset.good_sentences\n",
    "prompts_cf = dataset.bad_sentences\n",
    "\n",
    "resid_streams, labels = all_prompts_to_resid_streams_gt(prompts, prompts_cf, model, resid_type='heads', position='mean')\n",
    "\n",
    "ground_truth = [\n",
    "    (0, 3), (0, 5), (0, 1), (5, 5), (6, 1), (6, 9), (7, 10), (8, 11), (9, 1), # attention heads\n",
    "]\n",
    "\n",
    "# Save to ..data/gt folder\n",
    "torch.save(resid_streams, \"../data/gt/easy_negs/resid_heads_mean_2.pt\")\n",
    "all_prompts = prompts + prompts_cf\n",
    "torch.save(all_prompts, \"../data/gt/easy_negs/prompts_heads_mean.pt\")\n",
    "torch.save(labels, \"../data/gt/easy_negs/labels_heads_mean.pt\")\n",
    "torch.save(ground_truth, \"../data/gt/easy_negs/ground_truth.pt\")\n",
    "\n",
    "\n",
    "print(\"Greater-than task done.\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greater-than task...\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:23<00:00, 21.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greater-than task done.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "print(\"Greater-than task...\")\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "\n",
    "def generate_real_sentence(noun: str, year: int, eos: bool = False) -> str:\n",
    "    century = year // 100\n",
    "    sentence = f\"The {noun} lasted from the year {year} to the year {century}\"\n",
    "    if eos:\n",
    "        sentence = \" \" + sentence\n",
    "    return sentence\n",
    "\n",
    "def real_sentence_prompt(eos: bool = False) -> List[str]:\n",
    "    sentence = f\"The NOUN lasted from the year XX1 YY to the year XX2\".split()\n",
    "    if eos:\n",
    "        sentence = [\"\"] + sentence\n",
    "    return sentence\n",
    "\n",
    "def generate_bad_sentence(noun: str, year: int, eos: bool = False) -> str:\n",
    "    century = year // 100\n",
    "    sentence = f\"The {noun} lasted from the year {century}AB to the year {century}\"\n",
    "    if eos:\n",
    "        sentence = \" \" + sentence\n",
    "    return sentence\n",
    "\n",
    "def bad_sentence_prompt(eos: bool = False) -> List[str]:\n",
    "    sentence = f\"The NOUN lasted from the year XX1 01 to the year XX2\".split()\n",
    "    if eos:\n",
    "        sentence = [\"\"] + sentence\n",
    "    return sentence\n",
    "\n",
    "def is_valid_year(year: str, model) -> bool:\n",
    "    _year = \" \" + year\n",
    "    token = model.to_tokens(_year)\n",
    "    detok = model.to_string(token)\n",
    "    return len(detok) == 2 and len(detok[1]) == 2\n",
    "\n",
    "class YearDataset:\n",
    "    years_to_sample_from: torch.Tensor\n",
    "    N: int\n",
    "    ordered: bool\n",
    "    eos: bool\n",
    "\n",
    "    nouns: List[str]\n",
    "    years: torch.Tensor\n",
    "    years_YY: torch.Tensor\n",
    "    good_sentences: List[str]\n",
    "    bad_sentences: List[str]\n",
    "    good_toks: torch.Tensor\n",
    "    bad_toks: torch.Tensor\n",
    "    good_prompt: List[str]\n",
    "    bad_prompt: List[str]\n",
    "    good_mask: torch.Tensor\n",
    "    model: HookedTransformer\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        years_to_sample_from,\n",
    "        N: int,\n",
    "        nouns: Union[str, List[str], Path],\n",
    "        model: HookedTransformer,\n",
    "        balanced: bool = True,\n",
    "        eos: bool = False,\n",
    "        device: str = \"cpu\",\n",
    "    ):\n",
    "        self.years_to_sample_from = years_to_sample_from\n",
    "        self.N = N\n",
    "        self.eos = eos\n",
    "        self.model = model\n",
    "\n",
    "        if isinstance(nouns, str):\n",
    "            noun_list = [nouns]\n",
    "        elif isinstance(nouns, list):\n",
    "            noun_list = nouns\n",
    "        elif isinstance(nouns, Path):\n",
    "            with open(nouns, \"r\") as f:\n",
    "                noun_list = [line.strip() for line in f]\n",
    "        else:\n",
    "            raise ValueError(f\"Got bad type of nouns: {type(nouns)}; for nouns: {nouns}\")\n",
    "\n",
    "        self.nouns = random.choices(noun_list, k=N)\n",
    "\n",
    "        if balanced:\n",
    "            years = []\n",
    "            current_year = 2\n",
    "            years_to_sample_from_YY = self.years_to_sample_from % 100\n",
    "            for i in range(N):\n",
    "                sample_pool = self.years_to_sample_from[years_to_sample_from_YY == current_year]\n",
    "                years.append(sample_pool[random.randrange(len(sample_pool))])\n",
    "                current_year += 1\n",
    "                if current_year >= 99:\n",
    "                    current_year -= 97\n",
    "            self.years = torch.tensor(years)\n",
    "        else:\n",
    "            self.years = torch.tensor(self.years_to_sample_from[torch.randint(0, len(self.years_to_sample_from), (N,))])\n",
    "\n",
    "        self.years_XX = self.years // 100\n",
    "        self.years_YY = self.years % 100\n",
    "\n",
    "        self.good_sentences = [\n",
    "            generate_real_sentence(noun, int(year.item()), eos=eos) for noun, year in zip(self.nouns, self.years)\n",
    "        ]\n",
    "        self.bad_sentences = [\n",
    "            generate_bad_sentence(noun, int(year.item()), eos=eos) for noun, year in zip(self.nouns, self.years)\n",
    "        ]\n",
    "\n",
    "        self.good_prompt = real_sentence_prompt(eos=eos)\n",
    "        self.bad_prompt = bad_sentence_prompt(eos=eos)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.N\n",
    "\n",
    "# Instantiate the model\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\", device='cpu')\n",
    "\n",
    "# Define your nouns and years\n",
    "nouns = [\n",
    "    \"abduction\", \"accord\", \"affair\", \"agreement\", \"appraisal\",\n",
    "    \"assaults\", \"assessment\", \"attack\", \"attempts\", \"campaign\", \n",
    "    \"captivity\", \"case\", \"challenge\", \"chaos\", \"clash\", \n",
    "    \"collaboration\", \"coma\", \"competition\", \"confrontation\", \"consequence\", \n",
    "    \"conspiracy\", \"construction\", \"consultation\", \"contact\",\n",
    "    \"contract\", \"convention\", \"cooperation\", \"custody\", \"deal\", \n",
    "    \"decline\", \"decrease\", \"demonstrations\", \"development\", \"disagreement\", \n",
    "    \"disorder\", \"dispute\", \"domination\", \"dynasty\", \"effect\", \n",
    "    \"effort\", \"employment\", \"endeavor\", \"engagement\",\n",
    "    \"epidemic\", \"evaluation\", \"exchange\", \"existence\", \"expansion\", \n",
    "    \"expedition\", \"experiments\", \"fall\", \"fame\", \"flights\",\n",
    "    \"friendship\", \"growth\", \"hardship\", \"hostility\", \"illness\", \n",
    "    \"impact\", \"imprisonment\", \"improvement\", \"incarceration\",\n",
    "    \"increase\", \"insurgency\", \"invasion\", \"investigation\", \"journey\", \n",
    "]  # Example nouns list\n",
    "years_to_sample_from = torch.arange(1200, 2000)  # Example years range\n",
    "\n",
    "# Instantiate the YearDataset class\n",
    "dataset = YearDataset(\n",
    "    years_to_sample_from=years_to_sample_from,\n",
    "    N=250,  # Number of samples you want\n",
    "    nouns=nouns,\n",
    "    model=model,\n",
    "    balanced=True,  # Whether to balance the years in the dataset\n",
    "    eos=False,  # Whether to add an end-of-sentence token\n",
    "    device=\"cpu\"  # Device to use ('cpu' or 'cuda')\n",
    ")\n",
    "\n",
    "# Set up the model\n",
    "def prompt_to_resid_stream(prompt: str, model: HookedTransformer, resid_type: str = 'accumulated', position: str = 'last') -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convert a prompt to a residual stream of size (n_layers, d_model)\n",
    "    \"\"\"\n",
    "    # Run the model over the prompt\n",
    "    with torch.no_grad():\n",
    "        _, cache = model.run_with_cache(prompt)\n",
    "\n",
    "        # Get the accumulated residuals\n",
    "        if resid_type == 'accumulated':\n",
    "            resid, _ = cache.accumulated_resid(return_labels=True, apply_ln=True)\n",
    "        elif resid_type == 'decomposed':\n",
    "            resid, _ = cache.decompose_resid(return_labels=True)\n",
    "        elif resid_type == 'heads':\n",
    "            cache.compute_head_results()\n",
    "            head_resid, head_labels = cache.stack_head_results(return_labels=True)\n",
    "            #mlp_resid, mlp_labels = cache.decompose_resid(mode='mlp', incl_embeds=False, return_labels=True)\n",
    "            # Combine\n",
    "            # resid = torch.cat([head_resid, mlp_resid], dim=0)\n",
    "            # labels = head_labels + mlp_labels\n",
    "            resid = head_resid\n",
    "            labels = head_labels\n",
    "        else:\n",
    "            raise ValueError(\"resid_type must be one of 'accumulated', 'decomposed', 'heads'\")\n",
    "\n",
    "    # POSITION\n",
    "    if position == 'last':\n",
    "        last_token_accum = resid[:, 0, -1, :]  # layer, batch, pos, d_model\n",
    "    elif position == 'mean':\n",
    "        last_token_accum = resid.mean(dim=2).squeeze()\n",
    "    else:\n",
    "        raise ValueError(\"position must be one of 'last', 'mean'\")\n",
    "    return last_token_accum, labels\n",
    "\n",
    "\n",
    "def all_prompts_to_resid_streams_gt(prompts, prompts_cf, model, resid_type='accumulated', position='mean'):\n",
    "    \"\"\"\n",
    "    Convert all prompts and counterfactual prompts to residual streams\n",
    "    \"\"\"\n",
    "    # Stack prompts and prompts cf\n",
    "    resid_streams = []\n",
    "    # Combine the lists of strs\n",
    "    all_prompts = prompts + prompts_cf\n",
    "    for i in tqdm(range(len(all_prompts))):\n",
    "        prompt = all_prompts[i]\n",
    "        resid_stream, labels = prompt_to_resid_stream(prompt, model, resid_type, position)\n",
    "        resid_streams.append(resid_stream)\n",
    "    # Stack the residual streams into a single tensor\n",
    "    return torch.stack(resid_streams), labels\n",
    "\n",
    "prompts = dataset.good_sentences\n",
    "prompts_cf = dataset.bad_sentences\n",
    "\n",
    "resid_streams, labels = all_prompts_to_resid_streams_gt(prompts, prompts_cf, model, resid_type='heads', position='mean')\n",
    "\n",
    "ground_truth = [\n",
    "    (0, 3), (0, 5), (0, 1), (5, 5), (6, 1), (6, 9), (7, 10), (8, 11), (9, 1), # attention heads\n",
    "]\n",
    "\n",
    "# Save to ..data/gt folder\n",
    "torch.save(resid_streams, \"../data/gt/easy_negs/resid_heads_mean_3.pt\")\n",
    "all_prompts = prompts + prompts_cf\n",
    "torch.save(all_prompts, \"../data/gt/easy_negs/prompts_heads_mean.pt\")\n",
    "torch.save(labels, \"../data/gt/easy_negs/labels_heads_mean.pt\")\n",
    "torch.save(ground_truth, \"../data/gt/easy_negs/ground_truth.pt\")\n",
    "\n",
    "\n",
    "print(\"Greater-than task done.\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greater-than task...\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:22<00:00, 22.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greater-than task done.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "print(\"Greater-than task...\")\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "\n",
    "def generate_real_sentence(noun: str, year: int, eos: bool = False) -> str:\n",
    "    century = year // 100\n",
    "    sentence = f\"The {noun} lasted from the year {year} to the year {century}\"\n",
    "    if eos:\n",
    "        sentence = \" \" + sentence\n",
    "    return sentence\n",
    "\n",
    "def real_sentence_prompt(eos: bool = False) -> List[str]:\n",
    "    sentence = f\"The NOUN lasted from the year XX1 YY to the year XX2\".split()\n",
    "    if eos:\n",
    "        sentence = [\"\"] + sentence\n",
    "    return sentence\n",
    "\n",
    "def generate_bad_sentence(noun: str, year: int, eos: bool = False) -> str:\n",
    "    century = year // 100\n",
    "    sentence = f\"The {noun} lasted from the year {year+1} to the year {century}\"\n",
    "    if eos:\n",
    "        sentence = \" \" + sentence\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def bad_sentence_prompt(eos: bool = False) -> List[str]:\n",
    "    sentence = f\"The NOUN lasted from the year XX1 01 to the year XX2\".split()\n",
    "    if eos:\n",
    "        sentence = [\"\"] + sentence\n",
    "    return sentence\n",
    "\n",
    "def is_valid_year(year: str, model) -> bool:\n",
    "    _year = \" \" + year\n",
    "    token = model.to_tokens(_year)\n",
    "    detok = model.to_string(token)\n",
    "    return len(detok) == 2 and len(detok[1]) == 2\n",
    "\n",
    "class YearDataset:\n",
    "    years_to_sample_from: torch.Tensor\n",
    "    N: int\n",
    "    ordered: bool\n",
    "    eos: bool\n",
    "\n",
    "    nouns: List[str]\n",
    "    years: torch.Tensor\n",
    "    years_YY: torch.Tensor\n",
    "    good_sentences: List[str]\n",
    "    bad_sentences: List[str]\n",
    "    good_toks: torch.Tensor\n",
    "    bad_toks: torch.Tensor\n",
    "    good_prompt: List[str]\n",
    "    bad_prompt: List[str]\n",
    "    good_mask: torch.Tensor\n",
    "    model: HookedTransformer\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        years_to_sample_from,\n",
    "        N: int,\n",
    "        nouns: Union[str, List[str], Path],\n",
    "        model: HookedTransformer,\n",
    "        balanced: bool = True,\n",
    "        eos: bool = False,\n",
    "        device: str = \"cpu\",\n",
    "    ):\n",
    "        self.years_to_sample_from = years_to_sample_from\n",
    "        self.N = N\n",
    "        self.eos = eos\n",
    "        self.model = model\n",
    "\n",
    "        if isinstance(nouns, str):\n",
    "            noun_list = [nouns]\n",
    "        elif isinstance(nouns, list):\n",
    "            noun_list = nouns\n",
    "        elif isinstance(nouns, Path):\n",
    "            with open(nouns, \"r\") as f:\n",
    "                noun_list = [line.strip() for line in f]\n",
    "        else:\n",
    "            raise ValueError(f\"Got bad type of nouns: {type(nouns)}; for nouns: {nouns}\")\n",
    "\n",
    "        self.nouns = random.choices(noun_list, k=N)\n",
    "\n",
    "        if balanced:\n",
    "            years = []\n",
    "            current_year = 2\n",
    "            years_to_sample_from_YY = self.years_to_sample_from % 100\n",
    "            for i in range(N):\n",
    "                sample_pool = self.years_to_sample_from[years_to_sample_from_YY == current_year]\n",
    "                years.append(sample_pool[random.randrange(len(sample_pool))])\n",
    "                current_year += 1\n",
    "                if current_year >= 99:\n",
    "                    current_year -= 97\n",
    "            self.years = torch.tensor(years)\n",
    "        else:\n",
    "            self.years = torch.tensor(self.years_to_sample_from[torch.randint(0, len(self.years_to_sample_from), (N,))])\n",
    "\n",
    "        self.years_XX = self.years // 100\n",
    "        self.years_YY = self.years % 100\n",
    "\n",
    "        self.good_sentences = [\n",
    "            generate_real_sentence(noun, int(year.item()), eos=eos) for noun, year in zip(self.nouns, self.years)\n",
    "        ]\n",
    "        self.bad_sentences = [\n",
    "            generate_bad_sentence(noun, int(year.item()), eos=eos) for noun, year in zip(self.nouns, self.years)\n",
    "        ]\n",
    "\n",
    "        self.good_prompt = real_sentence_prompt(eos=eos)\n",
    "        self.bad_prompt = bad_sentence_prompt(eos=eos)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.N\n",
    "\n",
    "# Instantiate the model\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\", device='cpu')\n",
    "\n",
    "# Define your nouns and years\n",
    "nouns = [\n",
    "    \"abduction\", \"accord\", \"affair\", \"agreement\", \"appraisal\",\n",
    "    \"assaults\", \"assessment\", \"attack\", \"attempts\", \"campaign\", \n",
    "    \"captivity\", \"case\", \"challenge\", \"chaos\", \"clash\", \n",
    "    \"collaboration\", \"coma\", \"competition\", \"confrontation\", \"consequence\", \n",
    "    \"conspiracy\", \"construction\", \"consultation\", \"contact\",\n",
    "    \"contract\", \"convention\", \"cooperation\", \"custody\", \"deal\", \n",
    "    \"decline\", \"decrease\", \"demonstrations\", \"development\", \"disagreement\", \n",
    "    \"disorder\", \"dispute\", \"domination\", \"dynasty\", \"effect\", \n",
    "    \"effort\", \"employment\", \"endeavor\", \"engagement\",\n",
    "    \"epidemic\", \"evaluation\", \"exchange\", \"existence\", \"expansion\", \n",
    "    \"expedition\", \"experiments\", \"fall\", \"fame\", \"flights\",\n",
    "    \"friendship\", \"growth\", \"hardship\", \"hostility\", \"illness\", \n",
    "    \"impact\", \"imprisonment\", \"improvement\", \"incarceration\",\n",
    "    \"increase\", \"insurgency\", \"invasion\", \"investigation\", \"journey\", \n",
    "]  # Example nouns list\n",
    "years_to_sample_from = torch.arange(1200, 2000)  # Example years range\n",
    "\n",
    "# Instantiate the YearDataset class\n",
    "dataset = YearDataset(\n",
    "    years_to_sample_from=years_to_sample_from,\n",
    "    N=250,  # Number of samples you want\n",
    "    nouns=nouns,\n",
    "    model=model,\n",
    "    balanced=True,  # Whether to balance the years in the dataset\n",
    "    eos=False,  # Whether to add an end-of-sentence token\n",
    "    device=\"cpu\"  # Device to use ('cpu' or 'cuda')\n",
    ")\n",
    "\n",
    "# Set up the model\n",
    "def prompt_to_resid_stream(prompt: str, model: HookedTransformer, resid_type: str = 'accumulated', position: str = 'last') -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convert a prompt to a residual stream of size (n_layers, d_model)\n",
    "    \"\"\"\n",
    "    # Run the model over the prompt\n",
    "    with torch.no_grad():\n",
    "        _, cache = model.run_with_cache(prompt)\n",
    "\n",
    "        # Get the accumulated residuals\n",
    "        if resid_type == 'accumulated':\n",
    "            resid, _ = cache.accumulated_resid(return_labels=True, apply_ln=True)\n",
    "        elif resid_type == 'decomposed':\n",
    "            resid, _ = cache.decompose_resid(return_labels=True)\n",
    "        elif resid_type == 'heads':\n",
    "            cache.compute_head_results()\n",
    "            head_resid, head_labels = cache.stack_head_results(return_labels=True)\n",
    "            #mlp_resid, mlp_labels = cache.decompose_resid(mode='mlp', incl_embeds=False, return_labels=True)\n",
    "            # Combine\n",
    "            # resid = torch.cat([head_resid, mlp_resid], dim=0)\n",
    "            # labels = head_labels + mlp_labels\n",
    "            resid = head_resid\n",
    "            labels = head_labels\n",
    "        else:\n",
    "            raise ValueError(\"resid_type must be one of 'accumulated', 'decomposed', 'heads'\")\n",
    "\n",
    "    # POSITION\n",
    "    if position == 'last':\n",
    "        last_token_accum = resid[:, 0, -1, :]  # layer, batch, pos, d_model\n",
    "    elif position == 'mean':\n",
    "        last_token_accum = resid.mean(dim=2).squeeze()\n",
    "    else:\n",
    "        raise ValueError(\"position must be one of 'last', 'mean'\")\n",
    "    return last_token_accum, labels\n",
    "\n",
    "\n",
    "def all_prompts_to_resid_streams_gt(prompts, prompts_cf, model, resid_type='accumulated', position='mean'):\n",
    "    \"\"\"\n",
    "    Convert all prompts and counterfactual prompts to residual streams\n",
    "    \"\"\"\n",
    "    # Stack prompts and prompts cf\n",
    "    resid_streams = []\n",
    "    # Combine the lists of strs\n",
    "    all_prompts = prompts + prompts_cf\n",
    "    for i in tqdm(range(len(all_prompts))):\n",
    "        prompt = all_prompts[i]\n",
    "        resid_stream, labels = prompt_to_resid_stream(prompt, model, resid_type, position)\n",
    "        resid_streams.append(resid_stream)\n",
    "    # Stack the residual streams into a single tensor\n",
    "    return torch.stack(resid_streams), labels\n",
    "\n",
    "prompts = dataset.good_sentences\n",
    "prompts_cf = dataset.bad_sentences\n",
    "\n",
    "resid_streams, labels = all_prompts_to_resid_streams_gt(prompts, prompts_cf, model, resid_type='heads', position='mean')\n",
    "\n",
    "ground_truth = [\n",
    "    (0, 3), (0, 5), (0, 1), (5, 5), (6, 1), (6, 9), (7, 10), (8, 11), (9, 1), # attention heads\n",
    "]\n",
    "\n",
    "# Save to ..data/gt folder\n",
    "torch.save(resid_streams, \"../data/gt/easy_negs/resid_heads_mean_4.pt\")\n",
    "all_prompts = prompts + prompts_cf\n",
    "torch.save(all_prompts, \"../data/gt/easy_negs/prompts_heads_mean.pt\")\n",
    "torch.save(labels, \"../data/gt/easy_negs/labels_heads_mean.pt\")\n",
    "torch.save(ground_truth, \"../data/gt/easy_negs/ground_truth.pt\")\n",
    "\n",
    "\n",
    "print(\"Greater-than task done.\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The attack lasted from the year 1702 to the year 17 The attack lasted from the year 1702 to the year 17\n"
     ]
    }
   ],
   "source": [
    "print(prompts_cf[0], prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greater-than task...\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:23<00:00, 21.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greater-than task done.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "print(\"Greater-than task...\")\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "\n",
    "def generate_real_sentence(noun: str, year: int, eos: bool = False) -> str:\n",
    "    century = year // 100\n",
    "    sentence = f\"The {noun} lasted from the year {year} to the year {century}\"\n",
    "    if eos:\n",
    "        sentence = \" \" + sentence\n",
    "    return sentence\n",
    "\n",
    "def real_sentence_prompt(eos: bool = False) -> List[str]:\n",
    "    sentence = f\"The NOUN lasted from the year XX1 YY to the year XX2\".split()\n",
    "    if eos:\n",
    "        sentence = [\"\"] + sentence\n",
    "    return sentence\n",
    "\n",
    "def generate_bad_sentence(noun: str, year: int, eos: bool = False) -> str:\n",
    "    century = year // 100\n",
    "    sentence = f\"I've got a lovely bunch of {noun}\"\n",
    "    if eos:\n",
    "        sentence = \" \" + sentence\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def bad_sentence_prompt(eos: bool = False) -> List[str]:\n",
    "    sentence = f\"The NOUN lasted from the year XX1 01 to the year XX2\".split()\n",
    "    if eos:\n",
    "        sentence = [\"\"] + sentence\n",
    "    return sentence\n",
    "\n",
    "def is_valid_year(year: str, model) -> bool:\n",
    "    _year = \" \" + year\n",
    "    token = model.to_tokens(_year)\n",
    "    detok = model.to_string(token)\n",
    "    return len(detok) == 2 and len(detok[1]) == 2\n",
    "\n",
    "class YearDataset:\n",
    "    years_to_sample_from: torch.Tensor\n",
    "    N: int\n",
    "    ordered: bool\n",
    "    eos: bool\n",
    "\n",
    "    nouns: List[str]\n",
    "    years: torch.Tensor\n",
    "    years_YY: torch.Tensor\n",
    "    good_sentences: List[str]\n",
    "    bad_sentences: List[str]\n",
    "    good_toks: torch.Tensor\n",
    "    bad_toks: torch.Tensor\n",
    "    good_prompt: List[str]\n",
    "    bad_prompt: List[str]\n",
    "    good_mask: torch.Tensor\n",
    "    model: HookedTransformer\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        years_to_sample_from,\n",
    "        N: int,\n",
    "        nouns: Union[str, List[str], Path],\n",
    "        model: HookedTransformer,\n",
    "        balanced: bool = True,\n",
    "        eos: bool = False,\n",
    "        device: str = \"cpu\",\n",
    "    ):\n",
    "        self.years_to_sample_from = years_to_sample_from\n",
    "        self.N = N\n",
    "        self.eos = eos\n",
    "        self.model = model\n",
    "\n",
    "        if isinstance(nouns, str):\n",
    "            noun_list = [nouns]\n",
    "        elif isinstance(nouns, list):\n",
    "            noun_list = nouns\n",
    "        elif isinstance(nouns, Path):\n",
    "            with open(nouns, \"r\") as f:\n",
    "                noun_list = [line.strip() for line in f]\n",
    "        else:\n",
    "            raise ValueError(f\"Got bad type of nouns: {type(nouns)}; for nouns: {nouns}\")\n",
    "\n",
    "        self.nouns = random.choices(noun_list, k=N)\n",
    "\n",
    "        if balanced:\n",
    "            years = []\n",
    "            current_year = 2\n",
    "            years_to_sample_from_YY = self.years_to_sample_from % 100\n",
    "            for i in range(N):\n",
    "                sample_pool = self.years_to_sample_from[years_to_sample_from_YY == current_year]\n",
    "                years.append(sample_pool[random.randrange(len(sample_pool))])\n",
    "                current_year += 1\n",
    "                if current_year >= 99:\n",
    "                    current_year -= 97\n",
    "            self.years = torch.tensor(years)\n",
    "        else:\n",
    "            self.years = torch.tensor(self.years_to_sample_from[torch.randint(0, len(self.years_to_sample_from), (N,))])\n",
    "\n",
    "        self.years_XX = self.years // 100\n",
    "        self.years_YY = self.years % 100\n",
    "\n",
    "        self.good_sentences = [\n",
    "            generate_real_sentence(noun, int(year.item()), eos=eos) for noun, year in zip(self.nouns, self.years)\n",
    "        ]\n",
    "        self.bad_sentences = [\n",
    "            generate_bad_sentence(noun, int(year.item()), eos=eos) for noun, year in zip(self.nouns, self.years)\n",
    "        ]\n",
    "\n",
    "        self.good_prompt = real_sentence_prompt(eos=eos)\n",
    "        self.bad_prompt = bad_sentence_prompt(eos=eos)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.N\n",
    "\n",
    "# Instantiate the model\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\", device='cpu')\n",
    "\n",
    "# Define your nouns and years\n",
    "nouns = [\n",
    "    \"abduction\", \"accord\", \"affair\", \"agreement\", \"appraisal\",\n",
    "    \"assaults\", \"assessment\", \"attack\", \"attempts\", \"campaign\", \n",
    "    \"captivity\", \"case\", \"challenge\", \"chaos\", \"clash\", \n",
    "    \"collaboration\", \"coma\", \"competition\", \"confrontation\", \"consequence\", \n",
    "    \"conspiracy\", \"construction\", \"consultation\", \"contact\",\n",
    "    \"contract\", \"convention\", \"cooperation\", \"custody\", \"deal\", \n",
    "    \"decline\", \"decrease\", \"demonstrations\", \"development\", \"disagreement\", \n",
    "    \"disorder\", \"dispute\", \"domination\", \"dynasty\", \"effect\", \n",
    "    \"effort\", \"employment\", \"endeavor\", \"engagement\",\n",
    "    \"epidemic\", \"evaluation\", \"exchange\", \"existence\", \"expansion\", \n",
    "    \"expedition\", \"experiments\", \"fall\", \"fame\", \"flights\",\n",
    "    \"friendship\", \"growth\", \"hardship\", \"hostility\", \"illness\", \n",
    "    \"impact\", \"imprisonment\", \"improvement\", \"incarceration\",\n",
    "    \"increase\", \"insurgency\", \"invasion\", \"investigation\", \"journey\", \n",
    "]  # Example nouns list\n",
    "years_to_sample_from = torch.arange(1200, 2000)  # Example years range\n",
    "\n",
    "# Instantiate the YearDataset class\n",
    "dataset = YearDataset(\n",
    "    years_to_sample_from=years_to_sample_from,\n",
    "    N=250,  # Number of samples you want\n",
    "    nouns=nouns,\n",
    "    model=model,\n",
    "    balanced=True,  # Whether to balance the years in the dataset\n",
    "    eos=False,  # Whether to add an end-of-sentence token\n",
    "    device=\"cpu\"  # Device to use ('cpu' or 'cuda')\n",
    ")\n",
    "\n",
    "# Set up the model\n",
    "def prompt_to_resid_stream(prompt: str, model: HookedTransformer, resid_type: str = 'accumulated', position: str = 'last') -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convert a prompt to a residual stream of size (n_layers, d_model)\n",
    "    \"\"\"\n",
    "    # Run the model over the prompt\n",
    "    with torch.no_grad():\n",
    "        _, cache = model.run_with_cache(prompt)\n",
    "\n",
    "        # Get the accumulated residuals\n",
    "        if resid_type == 'accumulated':\n",
    "            resid, _ = cache.accumulated_resid(return_labels=True, apply_ln=True)\n",
    "        elif resid_type == 'decomposed':\n",
    "            resid, _ = cache.decompose_resid(return_labels=True)\n",
    "        elif resid_type == 'heads':\n",
    "            cache.compute_head_results()\n",
    "            head_resid, head_labels = cache.stack_head_results(return_labels=True)\n",
    "            #mlp_resid, mlp_labels = cache.decompose_resid(mode='mlp', incl_embeds=False, return_labels=True)\n",
    "            # Combine\n",
    "            # resid = torch.cat([head_resid, mlp_resid], dim=0)\n",
    "            # labels = head_labels + mlp_labels\n",
    "            resid = head_resid\n",
    "            labels = head_labels\n",
    "        else:\n",
    "            raise ValueError(\"resid_type must be one of 'accumulated', 'decomposed', 'heads'\")\n",
    "\n",
    "    # POSITION\n",
    "    if position == 'last':\n",
    "        last_token_accum = resid[:, 0, -1, :]  # layer, batch, pos, d_model\n",
    "    elif position == 'mean':\n",
    "        last_token_accum = resid.mean(dim=2).squeeze()\n",
    "    else:\n",
    "        raise ValueError(\"position must be one of 'last', 'mean'\")\n",
    "    return last_token_accum, labels\n",
    "\n",
    "\n",
    "def all_prompts_to_resid_streams_gt(prompts, prompts_cf, model, resid_type='accumulated', position='mean'):\n",
    "    \"\"\"\n",
    "    Convert all prompts and counterfactual prompts to residual streams\n",
    "    \"\"\"\n",
    "    # Stack prompts and prompts cf\n",
    "    resid_streams = []\n",
    "    # Combine the lists of strs\n",
    "    all_prompts = prompts + prompts_cf\n",
    "    for i in tqdm(range(len(all_prompts))):\n",
    "        prompt = all_prompts[i]\n",
    "        resid_stream, labels = prompt_to_resid_stream(prompt, model, resid_type, position)\n",
    "        resid_streams.append(resid_stream)\n",
    "    # Stack the residual streams into a single tensor\n",
    "    return torch.stack(resid_streams), labels\n",
    "\n",
    "prompts = dataset.good_sentences\n",
    "prompts_cf = dataset.bad_sentences\n",
    "\n",
    "resid_streams, labels = all_prompts_to_resid_streams_gt(prompts, prompts_cf, model, resid_type='heads', position='mean')\n",
    "\n",
    "ground_truth = [\n",
    "    (0, 3), (0, 5), (0, 1), (5, 5), (6, 1), (6, 9), (7, 10), (8, 11), (9, 1), # attention heads\n",
    "]\n",
    "\n",
    "# Save to ..data/gt folder\n",
    "torch.save(resid_streams, \"../data/gt/easy_negs/resid_heads_mean_5.pt\")\n",
    "all_prompts = prompts + prompts_cf\n",
    "torch.save(all_prompts, \"../data/gt/easy_negs/prompts_heads_mean.pt\")\n",
    "torch.save(labels, \"../data/gt/easy_negs/labels_heads_mean.pt\")\n",
    "torch.save(ground_truth, \"../data/gt/easy_negs/ground_truth.pt\")\n",
    "\n",
    "\n",
    "print(\"Greater-than task done.\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I've got a lovely bunch of domination\", \"I've got a lovely bunch of effect\", \"I've got a lovely bunch of consultation\", \"I've got a lovely bunch of attack\", \"I've got a lovely bunch of fall\", \"I've got a lovely bunch of exchange\", \"I've got a lovely bunch of hostility\", \"I've got a lovely bunch of domination\", \"I've got a lovely bunch of increase\", \"I've got a lovely bunch of conspiracy\", \"I've got a lovely bunch of consultation\", \"I've got a lovely bunch of journey\", \"I've got a lovely bunch of appraisal\", \"I've got a lovely bunch of captivity\", \"I've got a lovely bunch of captivity\", \"I've got a lovely bunch of existence\", \"I've got a lovely bunch of insurgency\", \"I've got a lovely bunch of confrontation\", \"I've got a lovely bunch of flights\", \"I've got a lovely bunch of challenge\", \"I've got a lovely bunch of consequence\", \"I've got a lovely bunch of cooperation\", \"I've got a lovely bunch of hostility\", \"I've got a lovely bunch of campaign\", \"I've got a lovely bunch of captivity\", \"I've got a lovely bunch of insurgency\", \"I've got a lovely bunch of incarceration\", \"I've got a lovely bunch of hardship\", \"I've got a lovely bunch of cooperation\", \"I've got a lovely bunch of demonstrations\", \"I've got a lovely bunch of fall\", \"I've got a lovely bunch of hardship\", \"I've got a lovely bunch of increase\", \"I've got a lovely bunch of hostility\", \"I've got a lovely bunch of competition\", \"I've got a lovely bunch of cooperation\", \"I've got a lovely bunch of fall\", \"I've got a lovely bunch of growth\", \"I've got a lovely bunch of consequence\", \"I've got a lovely bunch of clash\", \"I've got a lovely bunch of growth\", \"I've got a lovely bunch of attack\", \"I've got a lovely bunch of journey\", \"I've got a lovely bunch of fame\", \"I've got a lovely bunch of expansion\", \"I've got a lovely bunch of disorder\", \"I've got a lovely bunch of epidemic\", \"I've got a lovely bunch of challenge\", \"I've got a lovely bunch of engagement\", \"I've got a lovely bunch of competition\", \"I've got a lovely bunch of contract\", \"I've got a lovely bunch of engagement\", \"I've got a lovely bunch of assaults\", \"I've got a lovely bunch of journey\", \"I've got a lovely bunch of fall\", \"I've got a lovely bunch of contract\", \"I've got a lovely bunch of invasion\", \"I've got a lovely bunch of engagement\", \"I've got a lovely bunch of demonstrations\", \"I've got a lovely bunch of imprisonment\", \"I've got a lovely bunch of dispute\", \"I've got a lovely bunch of engagement\", \"I've got a lovely bunch of accord\", \"I've got a lovely bunch of attack\", \"I've got a lovely bunch of evaluation\", \"I've got a lovely bunch of challenge\", \"I've got a lovely bunch of case\", \"I've got a lovely bunch of attack\", \"I've got a lovely bunch of existence\", \"I've got a lovely bunch of challenge\", \"I've got a lovely bunch of accord\", \"I've got a lovely bunch of effect\", \"I've got a lovely bunch of campaign\", \"I've got a lovely bunch of attack\", \"I've got a lovely bunch of case\", \"I've got a lovely bunch of confrontation\", \"I've got a lovely bunch of agreement\", \"I've got a lovely bunch of journey\", \"I've got a lovely bunch of assaults\", \"I've got a lovely bunch of attack\", \"I've got a lovely bunch of conspiracy\", \"I've got a lovely bunch of affair\", \"I've got a lovely bunch of existence\", \"I've got a lovely bunch of abduction\", \"I've got a lovely bunch of construction\", \"I've got a lovely bunch of appraisal\", \"I've got a lovely bunch of hostility\", \"I've got a lovely bunch of engagement\", \"I've got a lovely bunch of decrease\", \"I've got a lovely bunch of coma\", \"I've got a lovely bunch of hardship\", \"I've got a lovely bunch of invasion\", \"I've got a lovely bunch of hardship\", \"I've got a lovely bunch of disorder\", \"I've got a lovely bunch of attack\", \"I've got a lovely bunch of captivity\", \"I've got a lovely bunch of confrontation\", \"I've got a lovely bunch of appraisal\", \"I've got a lovely bunch of contract\", \"I've got a lovely bunch of contact\", \"I've got a lovely bunch of fall\", \"I've got a lovely bunch of experiments\", \"I've got a lovely bunch of agreement\", \"I've got a lovely bunch of decline\", \"I've got a lovely bunch of contract\", \"I've got a lovely bunch of appraisal\", \"I've got a lovely bunch of disagreement\", \"I've got a lovely bunch of development\", \"I've got a lovely bunch of assessment\", \"I've got a lovely bunch of employment\", \"I've got a lovely bunch of campaign\", \"I've got a lovely bunch of evaluation\", \"I've got a lovely bunch of endeavor\", \"I've got a lovely bunch of expedition\", \"I've got a lovely bunch of attempts\", \"I've got a lovely bunch of assessment\", \"I've got a lovely bunch of construction\", \"I've got a lovely bunch of clash\", \"I've got a lovely bunch of contract\", \"I've got a lovely bunch of expansion\", \"I've got a lovely bunch of assessment\", \"I've got a lovely bunch of dynasty\", \"I've got a lovely bunch of collaboration\", \"I've got a lovely bunch of attack\", \"I've got a lovely bunch of disorder\", \"I've got a lovely bunch of hardship\", \"I've got a lovely bunch of confrontation\", \"I've got a lovely bunch of hardship\", \"I've got a lovely bunch of clash\", \"I've got a lovely bunch of development\", \"I've got a lovely bunch of deal\", \"I've got a lovely bunch of custody\", \"I've got a lovely bunch of appraisal\", \"I've got a lovely bunch of challenge\", \"I've got a lovely bunch of evaluation\", \"I've got a lovely bunch of employment\", \"I've got a lovely bunch of development\", \"I've got a lovely bunch of convention\", \"I've got a lovely bunch of investigation\", \"I've got a lovely bunch of existence\", \"I've got a lovely bunch of fall\", \"I've got a lovely bunch of disagreement\", \"I've got a lovely bunch of dynasty\", \"I've got a lovely bunch of increase\", \"I've got a lovely bunch of endeavor\", \"I've got a lovely bunch of fall\", \"I've got a lovely bunch of case\", \"I've got a lovely bunch of endeavor\", \"I've got a lovely bunch of deal\", \"I've got a lovely bunch of friendship\", \"I've got a lovely bunch of challenge\", \"I've got a lovely bunch of domination\", \"I've got a lovely bunch of clash\", \"I've got a lovely bunch of conspiracy\", \"I've got a lovely bunch of increase\", \"I've got a lovely bunch of increase\", \"I've got a lovely bunch of employment\", \"I've got a lovely bunch of attack\", \"I've got a lovely bunch of conspiracy\", \"I've got a lovely bunch of consultation\", \"I've got a lovely bunch of epidemic\", \"I've got a lovely bunch of dispute\", \"I've got a lovely bunch of collaboration\", \"I've got a lovely bunch of hostility\", \"I've got a lovely bunch of attempts\", \"I've got a lovely bunch of impact\", \"I've got a lovely bunch of growth\", \"I've got a lovely bunch of collaboration\", \"I've got a lovely bunch of fall\", \"I've got a lovely bunch of demonstrations\", \"I've got a lovely bunch of journey\", \"I've got a lovely bunch of abduction\", \"I've got a lovely bunch of assaults\", \"I've got a lovely bunch of appraisal\", \"I've got a lovely bunch of growth\", \"I've got a lovely bunch of contact\", \"I've got a lovely bunch of disorder\", \"I've got a lovely bunch of dynasty\", \"I've got a lovely bunch of cooperation\", \"I've got a lovely bunch of decline\", \"I've got a lovely bunch of hostility\", \"I've got a lovely bunch of existence\", \"I've got a lovely bunch of hardship\", \"I've got a lovely bunch of assessment\", \"I've got a lovely bunch of epidemic\", \"I've got a lovely bunch of hardship\", \"I've got a lovely bunch of growth\", \"I've got a lovely bunch of collaboration\", \"I've got a lovely bunch of employment\", \"I've got a lovely bunch of decline\", \"I've got a lovely bunch of consultation\", \"I've got a lovely bunch of convention\", \"I've got a lovely bunch of employment\", \"I've got a lovely bunch of consultation\", \"I've got a lovely bunch of agreement\", \"I've got a lovely bunch of growth\", \"I've got a lovely bunch of impact\", \"I've got a lovely bunch of confrontation\", \"I've got a lovely bunch of dispute\", \"I've got a lovely bunch of custody\", \"I've got a lovely bunch of demonstrations\", \"I've got a lovely bunch of engagement\", \"I've got a lovely bunch of growth\", \"I've got a lovely bunch of hardship\", \"I've got a lovely bunch of construction\", \"I've got a lovely bunch of dispute\", \"I've got a lovely bunch of flights\", \"I've got a lovely bunch of expansion\", \"I've got a lovely bunch of effect\", \"I've got a lovely bunch of friendship\", \"I've got a lovely bunch of contract\", \"I've got a lovely bunch of journey\", \"I've got a lovely bunch of effort\", \"I've got a lovely bunch of consequence\", \"I've got a lovely bunch of effort\", \"I've got a lovely bunch of decrease\", \"I've got a lovely bunch of effort\", \"I've got a lovely bunch of contract\", \"I've got a lovely bunch of chaos\", \"I've got a lovely bunch of convention\", \"I've got a lovely bunch of assessment\", \"I've got a lovely bunch of cooperation\", \"I've got a lovely bunch of abduction\", \"I've got a lovely bunch of decrease\", \"I've got a lovely bunch of imprisonment\", \"I've got a lovely bunch of case\", \"I've got a lovely bunch of illness\", \"I've got a lovely bunch of challenge\", \"I've got a lovely bunch of affair\", \"I've got a lovely bunch of flights\", \"I've got a lovely bunch of coma\", \"I've got a lovely bunch of contact\", \"I've got a lovely bunch of investigation\", \"I've got a lovely bunch of challenge\", \"I've got a lovely bunch of fall\", \"I've got a lovely bunch of existence\", \"I've got a lovely bunch of effect\", \"I've got a lovely bunch of attack\", \"I've got a lovely bunch of flights\", \"I've got a lovely bunch of disagreement\", \"I've got a lovely bunch of flights\", \"I've got a lovely bunch of improvement\", \"I've got a lovely bunch of increase\", \"I've got a lovely bunch of employment\", \"I've got a lovely bunch of domination\", \"I've got a lovely bunch of friendship\", \"I've got a lovely bunch of insurgency\", \"I've got a lovely bunch of decrease\", \"I've got a lovely bunch of deal\", \"I've got a lovely bunch of consequence\"]\n"
     ]
    }
   ],
   "source": [
    "print(prompts_cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
