{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Tracr` clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD compress a `tracr` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mtl_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjaxtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'batch pos'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjaxtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'batch pos d_model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'logits'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mloss_per_token\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mprepend_bos\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpadding_side\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLiteral\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'right'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mstart_at_layer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjaxtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'batch pos'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mshortformer_pos_embed\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjaxtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'batch pos d_model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mattention_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mstop_at_layer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpast_kv_cache\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformer_lens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpast_key_value_caching\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHookedTransformerKeyValueCache\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNoneType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjaxtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'batch pos d_vocab'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjaxtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjaxtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'batch pos-1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjaxtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'batch pos d_vocab'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjaxtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjaxtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'batch pos-1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSource:\u001b[0m   \n",
      "    \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mInt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"batch pos\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mFloat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"batch pos d_model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mreturn_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"logits\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mloss_per_token\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mprepend_bos\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUSE_DEFAULT_VALUE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mpadding_side\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLiteral\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"left\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"right\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUSE_DEFAULT_VALUE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mstart_at_layer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"batch pos\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mshortformer_pos_embed\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mFloat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"batch pos d_model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mattention_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# [batch pos]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mstop_at_layer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mpast_kv_cache\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mHookedTransformerKeyValueCache\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mFloat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"batch pos d_vocab\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mLoss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mFloat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"batch pos d_vocab\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLoss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Forward Pass.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Input is either a batch of tokens ([batch, pos]) or a text string, a string is automatically\u001b[0m\n",
      "\u001b[0;34m        tokenized to a batch of a single element. The prepend_bos flag only applies when inputting a\u001b[0m\n",
      "\u001b[0;34m        text string.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Note that loss is the standard \"predict the next token\" cross-entropy loss for GPT-2 style\u001b[0m\n",
      "\u001b[0;34m        language models - if you want a custom loss function, the recommended behaviour is returning\u001b[0m\n",
      "\u001b[0;34m        the logits and then applying your custom loss function.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Args:\u001b[0m\n",
      "\u001b[0;34m            return_type Optional[str]: The type of output to return. Can be one of: None (return\u001b[0m\n",
      "\u001b[0;34m                nothing, don't calculate logits), 'logits' (return logits), 'loss' (return\u001b[0m\n",
      "\u001b[0;34m                cross-entropy loss), 'both' (return logits and loss).\u001b[0m\n",
      "\u001b[0;34m            loss_per_token bool: Whether to return the (next token prediction) loss per token (True)\u001b[0m\n",
      "\u001b[0;34m                or average (False). Average loss is a scalar (averaged over position *and* batch),\u001b[0m\n",
      "\u001b[0;34m                per-token loss is a tensor ([batch, position-1]) - position-1 because we're\u001b[0m\n",
      "\u001b[0;34m                predicting the next token, and there's no specified next token for the final token.\u001b[0m\n",
      "\u001b[0;34m                Defaults to False.\u001b[0m\n",
      "\u001b[0;34m            prepend_bos Optional[bool]: Overrides self.cfg.default_prepend_bos. Whether to prepend\u001b[0m\n",
      "\u001b[0;34m                the BOS token to the input (only applies when input is a string). Defaults to None,\u001b[0m\n",
      "\u001b[0;34m                implying usage of self.cfg.default_prepend_bos which is set to True unless specified\u001b[0m\n",
      "\u001b[0;34m                otherwise. (Even for models not explicitly trained with a prepended BOS token, heads\u001b[0m\n",
      "\u001b[0;34m                often use the first position as a resting position and accordingly lose information\u001b[0m\n",
      "\u001b[0;34m                from the first token, so this empirically seems to give better results.) Pass True\u001b[0m\n",
      "\u001b[0;34m                or False to locally override the default.\u001b[0m\n",
      "\u001b[0;34m            padding_side Optional[Literal[\"left\", \"right\"]]: Overrides self.tokenizer.padding_side.\u001b[0m\n",
      "\u001b[0;34m                Specifies which side to pad on when tokenizing multiple strings of different\u001b[0m\n",
      "\u001b[0;34m                lengths.\u001b[0m\n",
      "\u001b[0;34m            start_at_layer Optional[int]: If not None, start the forward pass at the specified\u001b[0m\n",
      "\u001b[0;34m                layer. Requires input to be the residual stream before the specified layer with\u001b[0m\n",
      "\u001b[0;34m                shape [batch, pos, d_model]. Inclusive - ie, start_at_layer = 0 skips the embedding\u001b[0m\n",
      "\u001b[0;34m                then runs the rest of the model. Supports negative indexing. start_at_layer = -1\u001b[0m\n",
      "\u001b[0;34m                only runs the final block and the unembedding. Defaults to None (run the full\u001b[0m\n",
      "\u001b[0;34m                model).\u001b[0m\n",
      "\u001b[0;34m            tokens: Optional[Int[torch.Tensor, \"batch pos\"]]: Tokenized input. Only use if\u001b[0m\n",
      "\u001b[0;34m                start_at_layer is not None and return type is \"loss\" or \"both\".\u001b[0m\n",
      "\u001b[0;34m            shortformer_pos_embed: Optional[Float[torch.Tensor, \"batch pos d_model\"]]: Positional\u001b[0m\n",
      "\u001b[0;34m                embedding for shortformer models. Only use if start_at_layer is not None and\u001b[0m\n",
      "\u001b[0;34m                self.cfg.positional_embedding_type == \"shortformer\".\u001b[0m\n",
      "\u001b[0;34m            attention_mask: Optional[torch.Tensor]: The attention mask for padded tokens. Only use\u001b[0m\n",
      "\u001b[0;34m                if start_at_layer is not None and (self.tokenizer.padding_side == \"left\" or\u001b[0m\n",
      "\u001b[0;34m                past_kv_cache is not None).\u001b[0m\n",
      "\u001b[0;34m            stop_at_layer Optional[int]: If not None, stop the forward pass at the specified layer.\u001b[0m\n",
      "\u001b[0;34m                Exclusive - ie, stop_at_layer = 0 will only run the embedding layer, stop_at_layer =\u001b[0m\n",
      "\u001b[0;34m                1 will run the embedding layer and the first transformer block, etc. Supports\u001b[0m\n",
      "\u001b[0;34m                negative indexing. Useful for analysis of intermediate layers, eg finding neuron\u001b[0m\n",
      "\u001b[0;34m                activations in layer 3 of a 24 layer model. Defaults to None (run the full model).\u001b[0m\n",
      "\u001b[0;34m                If not None, we return the last residual stream computed.\u001b[0m\n",
      "\u001b[0;34m            past_kv_cache Optional[HookedTransformerKeyValueCache]: If not None, keys and values\u001b[0m\n",
      "\u001b[0;34m                will be stored for every attention head (unless the cache is frozen). If there are\u001b[0m\n",
      "\u001b[0;34m                keys and values already in the cache, these will be prepended to the keys and values\u001b[0m\n",
      "\u001b[0;34m                for the new input, so that the new tokens can pay attention to previous tokens. This\u001b[0m\n",
      "\u001b[0;34m                is useful for generating text, because we don't need to repeat computation for\u001b[0m\n",
      "\u001b[0;34m                tokens that have already been through the model. Also caches attention_mask so\u001b[0m\n",
      "\u001b[0;34m                previous tokens are masked correctly (unless frozen). Padding should be ignored in\u001b[0m\n",
      "\u001b[0;34m                all cases, so it's okay to eg. pass in left padded tokens twice in a row.\u001b[0m\n",
      "\u001b[0;34m                Warning: Don't accidently prepend_bos to the second half of a prompt.\u001b[0m\n",
      "\u001b[0;34m                Defaults to None (don't use caching).\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLocallyOverridenDefaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepend_bos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepend_bos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_side\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_side\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mstart_at_layer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mresidual\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mshortformer_pos_embed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_to_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mprepend_bos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepend_bos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mpadding_side\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_side\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mpast_kv_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_kv_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mstart_at_layer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mstart_at_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# If we explicitly want to start or stop at a layer, we only iterate through the blocks\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# between those indices. Note that start_at_layer is inclusive and stop_at_layer is\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# exclusive.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# Eg: start_at_layer==None + stop_at_layer==0 means to only run the embed.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# Eg: start_at_layer==3 + stop_at_layer==-1 means to run from layer 3 until the end of the PENULTIMATE layer\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mblocks_and_idxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblocks_and_idxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_at_layer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop_at_layer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;31m# Note that each block includes skip connections, so we don't need\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;31m# residual + block(residual)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;31m# If we're using multiple GPUs, we need to send the residual and shortformer_pos_embed to the correct GPU\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device_for_block_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32mif\u001b[0m \u001b[0mshortformer_pos_embed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mshortformer_pos_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshortformer_pos_embed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0mdevices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device_for_block_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mresidual\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;31m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;31m# block\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mpast_kv_cache_entry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_kv_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;32mif\u001b[0m \u001b[0mpast_kv_cache\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mshortformer_pos_embed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshortformer_pos_embed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m)\u001b[0m  \u001b[0;31m# [batch, pos, d_model]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mstop_at_layer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;31m# When we stop at an early layer, we end here rather than doing further computation\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32mreturn\u001b[0m \u001b[0mresidual\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalization_type\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_final\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresidual\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [batch, pos, d_model]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mreturn_type\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munembed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresidual\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [batch, pos, d_vocab]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32mif\u001b[0m \u001b[0mreturn_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"logits\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0mtokens\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tokens must be passed in if return_type is 'loss' or 'both'\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mper_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_per_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;32mif\u001b[0m \u001b[0mreturn_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;32melif\u001b[0m \u001b[0mreturn_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"both\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0;32mreturn\u001b[0m \u001b[0mOutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\u001b[0m\u001b[0;34mInvalid return_type passed in: \u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mreturn_type\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/envs/anu/lib/python3.12/site-packages/transformer_lens/HookedTransformer.py\n",
      "\u001b[0;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "tl_model.forward??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a `tracr` model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['pos_embed.W_pos', 'embed.W_E', 'unembed.W_U', 'blocks.0.attn.W_K', 'blocks.0.attn.b_K', 'blocks.0.attn.W_Q', 'blocks.0.attn.b_Q', 'blocks.0.attn.W_V', 'blocks.0.attn.b_V', 'blocks.0.attn.W_O', 'blocks.0.attn.b_O', 'blocks.0.mlp.W_in', 'blocks.0.mlp.b_in', 'blocks.0.mlp.W_out', 'blocks.0.mlp.b_out', 'blocks.1.attn.W_K', 'blocks.1.attn.b_K', 'blocks.1.attn.W_Q', 'blocks.1.attn.b_Q', 'blocks.1.attn.W_V', 'blocks.1.attn.b_V', 'blocks.1.attn.W_O', 'blocks.1.attn.b_O', 'blocks.1.mlp.W_in', 'blocks.1.mlp.b_in', 'blocks.1.mlp.W_out', 'blocks.1.mlp.b_out', 'blocks.2.attn.W_K', 'blocks.2.attn.b_K', 'blocks.2.attn.W_Q', 'blocks.2.attn.b_Q', 'blocks.2.attn.W_V', 'blocks.2.attn.b_V', 'blocks.2.attn.W_O', 'blocks.2.attn.b_O', 'blocks.2.mlp.W_in', 'blocks.2.mlp.b_in', 'blocks.2.mlp.W_out', 'blocks.2.mlp.b_out', 'blocks.3.attn.W_K', 'blocks.3.attn.b_K', 'blocks.3.attn.W_Q', 'blocks.3.attn.b_Q', 'blocks.3.attn.W_V', 'blocks.3.attn.b_V', 'blocks.3.attn.W_O', 'blocks.3.attn.b_O', 'blocks.3.mlp.W_in', 'blocks.3.mlp.b_in', 'blocks.3.mlp.W_out', 'blocks.3.mlp.b_out'])\n",
      "Moving model to device:  cpu\n",
      "Original Decoding: ['BOS', 2, 3, 2, 1]\n",
      "tensor([[[5.3889e-07, 1.0778e-06, 5.3889e-07, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [2.9040e-13, 1.0000e+00, 2.9040e-13, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [2.9040e-13, 5.8080e-13, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [2.9040e-13, 1.0000e+00, 2.9040e-13, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [1.0000e+00, 5.8080e-13, 2.9040e-13, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00]]], grad_fn=<AddBackward0>)\n",
      "TransformerLens Replicated Decoding: ['BOS', 2, 3, 2, 1]\n",
      "tensor([[2, 1, 3, 5, 1, 3],\n",
      "        [3, 4, 2, 5, 6, 2],\n",
      "        [2, 5, 1, 5, 1, 1],\n",
      "        ...,\n",
      "        [4, 1, 4, 2, 3, 4],\n",
      "        [1, 4, 4, 2, 5, 4],\n",
      "        [3, 2, 3, 5, 2, 2]])\n",
      "torch.Size([46656, 6])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../tracr')\n",
    "\n",
    "from tracr.rasp import rasp\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "def make_length():\n",
    "  all_true_selector = rasp.Select(rasp.tokens, rasp.tokens, rasp.Comparison.TRUE)\n",
    "  return rasp.SelectorWidth(all_true_selector)\n",
    "\n",
    "length = make_length()  # `length` is not a primitive in our implementation.\n",
    "opp_index = length - rasp.indices - 1\n",
    "flip = rasp.Select(rasp.indices, opp_index, rasp.Comparison.EQ)\n",
    "reverse = rasp.Aggregate(flip, rasp.tokens)\n",
    "\n",
    "from tracr.compiler import compiling\n",
    "\n",
    "bos = \"BOS\"\n",
    "model = compiling.compile_rasp_to_model(\n",
    "    reverse,\n",
    "    vocab={1, 2, 3, 4, 5, 6},\n",
    "    max_seq_len=5,\n",
    "    compiler_bos=bos,\n",
    ")\n",
    "\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig\n",
    "import einops\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def tracr_to_tl(model):\n",
    "\n",
    "    n_heads = model.model_config.num_heads\n",
    "    n_layers = model.model_config.num_layers\n",
    "    d_head = model.model_config.key_size\n",
    "    d_mlp = model.model_config.mlp_hidden_size\n",
    "    act_fn = \"relu\"\n",
    "    normalization_type = \"LN\"  if model.model_config.layer_norm else None\n",
    "    attention_type = \"causal\"  if model.model_config.causal else \"bidirectional\"\n",
    "\n",
    "\n",
    "    n_ctx = model.params[\"pos_embed\"]['embeddings'].shape[0]\n",
    "    # Equivalent to length of vocab, with BOS and PAD at the end\n",
    "    d_vocab = model.params[\"token_embed\"]['embeddings'].shape[0]\n",
    "    # Residual stream width, I don't know of an easy way to infer it from the above config.\n",
    "    d_model = model.params[\"token_embed\"]['embeddings'].shape[1]\n",
    "\n",
    "    # Equivalent to length of vocab, WITHOUT BOS and PAD at the end because we never care about these outputs\n",
    "    # In practice, we always feed the logits into an argmax\n",
    "    d_vocab_out = model.params[\"token_embed\"]['embeddings'].shape[0] - 2\n",
    "\n",
    "    cfg = HookedTransformerConfig(\n",
    "        n_layers=n_layers,\n",
    "        d_model=d_model,\n",
    "        d_head=d_head,\n",
    "        n_ctx=n_ctx,\n",
    "        d_vocab=d_vocab,\n",
    "        d_vocab_out=d_vocab_out,\n",
    "        d_mlp=d_mlp,\n",
    "        n_heads=n_heads,\n",
    "        act_fn=act_fn,\n",
    "        attention_dir=attention_type,\n",
    "        normalization_type=normalization_type,\n",
    "    )\n",
    "    tl_model = HookedTransformer(cfg)\n",
    "\n",
    "\n",
    "    # %%\n",
    "    sd = {}\n",
    "    sd[\"pos_embed.W_pos\"] = model.params[\"pos_embed\"]['embeddings']\n",
    "    sd[\"embed.W_E\"] = model.params[\"token_embed\"]['embeddings']\n",
    "    # Equivalent to max_seq_len plus one, for the BOS\n",
    "\n",
    "    # The unembed is just a projection onto the first few elements of the residual stream, these store output tokens\n",
    "    # This is a NumPy array, the rest are Jax Arrays, but w/e it's fine.\n",
    "    sd[\"unembed.W_U\"] = np.eye(d_model, d_vocab_out)\n",
    "\n",
    "    for l in range(n_layers):\n",
    "        sd[f\"blocks.{l}.attn.W_K\"] = einops.rearrange(\n",
    "            model.params[f\"transformer/layer_{l}/attn/key\"][\"w\"],\n",
    "            \"d_model (n_heads d_head) -> n_heads d_model d_head\",\n",
    "            d_head = d_head,\n",
    "            n_heads = n_heads\n",
    "        )\n",
    "        sd[f\"blocks.{l}.attn.b_K\"] = einops.rearrange(\n",
    "            model.params[f\"transformer/layer_{l}/attn/key\"][\"b\"],\n",
    "            \"(n_heads d_head) -> n_heads d_head\",\n",
    "            d_head = d_head,\n",
    "            n_heads = n_heads\n",
    "        )\n",
    "        sd[f\"blocks.{l}.attn.W_Q\"] = einops.rearrange(\n",
    "            model.params[f\"transformer/layer_{l}/attn/query\"][\"w\"],\n",
    "            \"d_model (n_heads d_head) -> n_heads d_model d_head\",\n",
    "            d_head = d_head,\n",
    "            n_heads = n_heads\n",
    "        )\n",
    "        sd[f\"blocks.{l}.attn.b_Q\"] = einops.rearrange(\n",
    "            model.params[f\"transformer/layer_{l}/attn/query\"][\"b\"],\n",
    "            \"(n_heads d_head) -> n_heads d_head\",\n",
    "            d_head = d_head,\n",
    "            n_heads = n_heads\n",
    "        )\n",
    "        sd[f\"blocks.{l}.attn.W_V\"] = einops.rearrange(\n",
    "            model.params[f\"transformer/layer_{l}/attn/value\"][\"w\"],\n",
    "            \"d_model (n_heads d_head) -> n_heads d_model d_head\",\n",
    "            d_head = d_head,\n",
    "            n_heads = n_heads\n",
    "        )\n",
    "        sd[f\"blocks.{l}.attn.b_V\"] = einops.rearrange(\n",
    "            model.params[f\"transformer/layer_{l}/attn/value\"][\"b\"],\n",
    "            \"(n_heads d_head) -> n_heads d_head\",\n",
    "            d_head = d_head,\n",
    "            n_heads = n_heads\n",
    "        )\n",
    "        sd[f\"blocks.{l}.attn.W_O\"] = einops.rearrange(\n",
    "            model.params[f\"transformer/layer_{l}/attn/linear\"][\"w\"],\n",
    "            \"(n_heads d_head) d_model -> n_heads d_head d_model\",\n",
    "            d_head = d_head,\n",
    "            n_heads = n_heads\n",
    "        )\n",
    "        sd[f\"blocks.{l}.attn.b_O\"] = model.params[f\"transformer/layer_{l}/attn/linear\"][\"b\"]\n",
    "\n",
    "        sd[f\"blocks.{l}.mlp.W_in\"] = model.params[f\"transformer/layer_{l}/mlp/linear_1\"][\"w\"]\n",
    "        sd[f\"blocks.{l}.mlp.b_in\"] = model.params[f\"transformer/layer_{l}/mlp/linear_1\"][\"b\"]\n",
    "        sd[f\"blocks.{l}.mlp.W_out\"] = model.params[f\"transformer/layer_{l}/mlp/linear_2\"][\"w\"]\n",
    "        sd[f\"blocks.{l}.mlp.b_out\"] = model.params[f\"transformer/layer_{l}/mlp/linear_2\"][\"b\"]\n",
    "    print(sd.keys())\n",
    "\n",
    "\n",
    "    for k, v in sd.items():\n",
    "        # I cannot figure out a neater way to go from a Jax array to a numpy array lol\n",
    "        sd[k] = torch.tensor(np.array(v))\n",
    "\n",
    "    tl_model.load_state_dict(sd, strict=False)\n",
    "\n",
    "    return tl_model\n",
    "\n",
    "tl_model = tracr_to_tl(model).to(device)\n",
    "\n",
    "INPUT_ENCODER = model.input_encoder\n",
    "OUTPUT_ENCODER = model.output_encoder\n",
    "\n",
    "def create_model_input(input, input_encoder=INPUT_ENCODER):\n",
    "    encoding = input_encoder.encode(input)\n",
    "    return torch.tensor(encoding).unsqueeze(dim=0)\n",
    "\n",
    "def decode_model_output(logits, output_encoder=OUTPUT_ENCODER, bos_token=INPUT_ENCODER.bos_token):\n",
    "    max_output_indices = logits.squeeze(dim=0).argmax(dim=-1)\n",
    "    decoded_output = output_encoder.decode(max_output_indices.tolist())\n",
    "    decoded_output_with_bos = [bos_token] + decoded_output[1:]\n",
    "    return decoded_output_with_bos\n",
    "\n",
    "input = [bos, 1, 2, 3, 2]\n",
    "out = model.apply(input)\n",
    "print(\"Original Decoding:\", out.decoded)\n",
    "\n",
    "input_tokens_tensor = create_model_input(input)\n",
    "logits = tl_model(input_tokens_tensor)\n",
    "print(logits)\n",
    "decoded_output = decode_model_output(logits)\n",
    "print(\"TransformerLens Replicated Decoding:\", decoded_output)\n",
    "\n",
    "# Randomly initialise all weights in the model\n",
    "#tl_model.init_weights()\n",
    "\n",
    "logits = tl_model(input_tokens_tensor)\n",
    "logits.squeeze(dim=0).max(dim=-1)\n",
    "\n",
    "# Target is reverse input tokens tensor (shape 1x5)\n",
    "idx = [i for i in range(input_tokens_tensor.size(1)-1, -1, -1)]\n",
    "idx = torch.LongTensor(idx)\n",
    "target = input_tokens_tensor[:, idx].type(torch.float32)\n",
    "target\n",
    "\n",
    "# Calculate L2 loss between logits and target\n",
    "criterion = torch.nn.MSELoss()\n",
    "loss = criterion(logits.squeeze(dim=0).argmax(dim=-1).cpu().type(torch.float32), target.squeeze().cpu())\n",
    "loss\n",
    "\n",
    "import torch\n",
    "import itertools\n",
    "\n",
    "def permutations_with_replacement_to_tensor(s):\n",
    "    # Convert the set to a list to allow indexed access\n",
    "    elements = list(s)\n",
    "    n = len(elements)\n",
    "    \n",
    "    # Generate all permutations with replacement using itertools.product\n",
    "    # This creates an iterator for all n-length combinations of the elements\n",
    "    all_permutations = itertools.product(elements, repeat=n)\n",
    "    \n",
    "    # Convert iterator to list of lists\n",
    "    permutations_list = [list(perm) for perm in all_permutations]\n",
    "    \n",
    "    # Convert list of lists to a PyTorch tensor\n",
    "    # Each permutation is a row in the tensor\n",
    "    permutations_tensor = torch.tensor(permutations_list)\n",
    "\n",
    "    # Shuffle tensor\n",
    "    perm_indices = torch.randperm(permutations_tensor.size(0))\n",
    "    permutations_tensor = permutations_tensor[perm_indices]\n",
    "    \n",
    "    return permutations_tensor\n",
    "\n",
    "# Example usage\n",
    "s = {1, 2, 3, 4, 5, 6}\n",
    "tensor = permutations_with_replacement_to_tensor(s)\n",
    "print(tensor)\n",
    "print(tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Decoding: ['BOS', 2, 3, 2, 1]\n",
      "tensor([[[5.3889e-07, 1.0778e-06, 5.3889e-07, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [2.9040e-13, 1.0000e+00, 2.9040e-13, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [2.9040e-13, 5.8080e-13, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [2.9040e-13, 1.0000e+00, 2.9040e-13, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [1.0000e+00, 5.8080e-13, 2.9040e-13, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00]]], grad_fn=<AddBackward0>)\n",
      "TransformerLens Replicated Decoding: ['BOS', 2, 3, 2, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2, 2, 3, 2, 1]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = [bos, 1, 2, 3, 2]\n",
    "out = model.apply(input)\n",
    "print(\"Original Decoding:\", out.decoded)\n",
    "\n",
    "input_tokens_tensor = create_model_input(input)\n",
    "logits = tl_model(input_tokens_tensor)\n",
    "print(logits)\n",
    "decoded_output = decode_model_output(logits)\n",
    "print(\"TransformerLens Replicated Decoding:\", decoded_output)\n",
    "\n",
    "logits = tl_model(input_tokens_tensor)\n",
    "max_output_indices = logits.squeeze(dim=0).argmax(dim=-1)\n",
    "OUTPUT_ENCODER.decode(max_output_indices.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 1., 0., 6.]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Target is reverse input tokens tensor (shape 1x5)\n",
    "idx = [i for i in range(input_tokens_tensor.size(1)-1, -1, -1)]\n",
    "idx = torch.LongTensor(idx)\n",
    "target = input_tokens_tensor[:, idx].type(torch.float32)\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 2., 2., 5.])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.squeeze(dim=0).argmax(dim=-1).cpu().type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.squeeze().cpu().type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2000)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate L2 loss between logits and target\n",
    "criterion = torch.nn.MSELoss()\n",
    "loss = criterion(logits.squeeze(dim=0).argmax(dim=-1).cpu().type(torch.float32), target.squeeze().cpu())\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([7]), torch.Size([7]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.squeeze(dim=0).argmax(dim=-1).cpu().type(torch.float32).shape, target.squeeze().cpu().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['pos_embed.W_pos', 'embed.W_E', 'unembed.W_U', 'blocks.0.attn.W_K', 'blocks.0.attn.b_K', 'blocks.0.attn.W_Q', 'blocks.0.attn.b_Q', 'blocks.0.attn.W_V', 'blocks.0.attn.b_V', 'blocks.0.attn.W_O', 'blocks.0.attn.b_O', 'blocks.0.mlp.W_in', 'blocks.0.mlp.b_in', 'blocks.0.mlp.W_out', 'blocks.0.mlp.b_out', 'blocks.1.attn.W_K', 'blocks.1.attn.b_K', 'blocks.1.attn.W_Q', 'blocks.1.attn.b_Q', 'blocks.1.attn.W_V', 'blocks.1.attn.b_V', 'blocks.1.attn.W_O', 'blocks.1.attn.b_O', 'blocks.1.mlp.W_in', 'blocks.1.mlp.b_in', 'blocks.1.mlp.W_out', 'blocks.1.mlp.b_out', 'blocks.2.attn.W_K', 'blocks.2.attn.b_K', 'blocks.2.attn.W_Q', 'blocks.2.attn.b_Q', 'blocks.2.attn.W_V', 'blocks.2.attn.b_V', 'blocks.2.attn.W_O', 'blocks.2.attn.b_O', 'blocks.2.mlp.W_in', 'blocks.2.mlp.b_in', 'blocks.2.mlp.W_out', 'blocks.2.mlp.b_out', 'blocks.3.attn.W_K', 'blocks.3.attn.b_K', 'blocks.3.attn.W_Q', 'blocks.3.attn.b_Q', 'blocks.3.attn.W_V', 'blocks.3.attn.b_V', 'blocks.3.attn.W_O', 'blocks.3.attn.b_O', 'blocks.3.mlp.W_in', 'blocks.3.mlp.b_in', 'blocks.3.mlp.W_out', 'blocks.3.mlp.b_out'])\n",
      "Moving model to device:  cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charlesoneill/miniconda3/envs/anu/lib/python3.12/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([7])) that is different to the input size (torch.Size([7, 6])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (6) must match the size of tensor b (7) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 86\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Convert target to float32 tensor and move to CPU\u001b[39;00m\n\u001b[1;32m     84\u001b[0m target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m---> 86\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     89\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/anu/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/anu/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/anu/lib/python3.12/site-packages/torch/nn/modules/loss.py:535\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/anu/lib/python3.12/site-packages/torch/nn/functional.py:3338\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3336\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3338\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3339\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[0;32m~/miniconda3/envs/anu/lib/python3.12/site-packages/torch/functional.py:76\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (6) must match the size of tensor b (7) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../tracr')\n",
    "\n",
    "from tracr.rasp import rasp\n",
    "from tracr.compiler import compiling\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig\n",
    "import einops\n",
    "import torch\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "def make_length():\n",
    "    all_true_selector = rasp.Select(rasp.tokens, rasp.tokens, rasp.Comparison.TRUE)\n",
    "    return rasp.SelectorWidth(all_true_selector)\n",
    "\n",
    "length = make_length()\n",
    "opp_index = length - rasp.indices - 1\n",
    "flip = rasp.Select(rasp.indices, opp_index, rasp.Comparison.EQ)\n",
    "reverse = rasp.Aggregate(flip, rasp.tokens)\n",
    "\n",
    "bos = \"BOS\"\n",
    "model = compiling.compile_rasp_to_model(\n",
    "    reverse,\n",
    "    vocab={1, 2, 3, 4, 5, 6},\n",
    "    max_seq_len=6,\n",
    "    compiler_bos=bos,\n",
    ")\n",
    "\n",
    "tl_model = tracr_to_tl(model).to(device)\n",
    "\n",
    "INPUT_ENCODER = model.input_encoder\n",
    "OUTPUT_ENCODER = model.output_encoder\n",
    "\n",
    "def create_model_input(input, input_encoder=INPUT_ENCODER):\n",
    "    encoding = input_encoder.encode(input)\n",
    "    return torch.tensor(encoding).unsqueeze(dim=0)\n",
    "\n",
    "def decode_model_output(logits, output_encoder=OUTPUT_ENCODER, bos_token=INPUT_ENCODER.bos_token):\n",
    "    max_output_indices = logits.squeeze(dim=0).argmax(dim=-1)\n",
    "    decoded_output = output_encoder.decode(max_output_indices.tolist())\n",
    "    decoded_output_with_bos = [bos_token] + decoded_output[1:]\n",
    "    return decoded_output_with_bos\n",
    "\n",
    "def permutations_with_replacement_to_tensor(s):\n",
    "    elements = list(s)\n",
    "    n = len(elements)\n",
    "    all_permutations = itertools.product(elements, repeat=n)\n",
    "    permutations_list = [list(perm) for perm in all_permutations]\n",
    "    permutations_tensor = torch.tensor(permutations_list)\n",
    "    perm_indices = torch.randperm(permutations_tensor.size(0))\n",
    "    permutations_tensor = permutations_tensor[perm_indices]\n",
    "    return permutations_tensor\n",
    "\n",
    "# Randomly initialize all weights in the model\n",
    "tl_model.init_weights()\n",
    "\n",
    "# Generate all permutations of the input\n",
    "input_permutations = permutations_with_replacement_to_tensor({1, 2, 3, 4, 5, 6})\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(tl_model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, input_permutations.size(0), batch_size):\n",
    "        batch = input_permutations[i:i+batch_size]\n",
    "        input_tokens_tensor = create_model_input([bos] + batch.tolist()[0])\n",
    "        logits = tl_model(input_tokens_tensor)\n",
    "        \n",
    "        idx = [i for i in range(input_tokens_tensor.size(1)-1, -1, -1)]\n",
    "        idx = torch.LongTensor(idx)\n",
    "        target = input_tokens_tensor[:, idx].type(torch.float32)\n",
    "        \n",
    "        # Convert logits to float32 and move to CPU\n",
    "        logits = logits.squeeze(dim=0).cpu().type(torch.float32)\n",
    "        \n",
    "        # Convert target to float32 tensor and move to CPU\n",
    "        target = target.squeeze().cpu().type(torch.float32)\n",
    "        \n",
    "        loss = criterion(logits, target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tracr-reverse`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../tracr')\n",
    "\n",
    "from tracr.rasp import rasp\n",
    "\n",
    "def make_length():\n",
    "  all_true_selector = rasp.Select(rasp.tokens, rasp.tokens, rasp.Comparison.TRUE)\n",
    "  return rasp.SelectorWidth(all_true_selector)\n",
    "\n",
    "length = make_length()  # `length` is not a primitive in our implementation.\n",
    "opp_index = length - rasp.indices - 1\n",
    "flip = rasp.Select(rasp.indices, opp_index, rasp.Comparison.EQ)\n",
    "reverse = rasp.Aggregate(flip, rasp.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tracr.compiler import compiling\n",
    "\n",
    "bos = \"BOS\"\n",
    "model = compiling.compile_rasp_to_model(\n",
    "    reverse,\n",
    "    vocab={1, 2, 3, 4, 5, 6},\n",
    "    max_seq_len=5,\n",
    "    compiler_bos=bos,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['pos_embed.W_pos', 'embed.W_E', 'unembed.W_U', 'blocks.0.attn.W_K', 'blocks.0.attn.b_K', 'blocks.0.attn.W_Q', 'blocks.0.attn.b_Q', 'blocks.0.attn.W_V', 'blocks.0.attn.b_V', 'blocks.0.attn.W_O', 'blocks.0.attn.b_O', 'blocks.0.mlp.W_in', 'blocks.0.mlp.b_in', 'blocks.0.mlp.W_out', 'blocks.0.mlp.b_out', 'blocks.1.attn.W_K', 'blocks.1.attn.b_K', 'blocks.1.attn.W_Q', 'blocks.1.attn.b_Q', 'blocks.1.attn.W_V', 'blocks.1.attn.b_V', 'blocks.1.attn.W_O', 'blocks.1.attn.b_O', 'blocks.1.mlp.W_in', 'blocks.1.mlp.b_in', 'blocks.1.mlp.W_out', 'blocks.1.mlp.b_out', 'blocks.2.attn.W_K', 'blocks.2.attn.b_K', 'blocks.2.attn.W_Q', 'blocks.2.attn.b_Q', 'blocks.2.attn.W_V', 'blocks.2.attn.b_V', 'blocks.2.attn.W_O', 'blocks.2.attn.b_O', 'blocks.2.mlp.W_in', 'blocks.2.mlp.b_in', 'blocks.2.mlp.W_out', 'blocks.2.mlp.b_out', 'blocks.3.attn.W_K', 'blocks.3.attn.b_K', 'blocks.3.attn.W_Q', 'blocks.3.attn.b_Q', 'blocks.3.attn.W_V', 'blocks.3.attn.b_V', 'blocks.3.attn.W_O', 'blocks.3.attn.b_O', 'blocks.3.mlp.W_in', 'blocks.3.mlp.b_in', 'blocks.3.mlp.W_out', 'blocks.3.mlp.b_out'])\n",
      "Original Decoding: ['BOS', 3, 2, 1]\n",
      "TransformerLens Replicated Decoding: ['BOS', 3, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer, HookedTransformerConfig\n",
    "import einops\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def tracr_to_tl(model):\n",
    "\n",
    "    n_heads = model.model_config.num_heads\n",
    "    n_layers = model.model_config.num_layers\n",
    "    d_head = model.model_config.key_size\n",
    "    d_mlp = model.model_config.mlp_hidden_size\n",
    "    act_fn = \"relu\"\n",
    "    normalization_type = \"LN\"  if model.model_config.layer_norm else None\n",
    "    attention_type = \"causal\"  if model.model_config.causal else \"bidirectional\"\n",
    "\n",
    "\n",
    "    n_ctx = model.params[\"pos_embed\"]['embeddings'].shape[0]\n",
    "    # Equivalent to length of vocab, with BOS and PAD at the end\n",
    "    d_vocab = model.params[\"token_embed\"]['embeddings'].shape[0]\n",
    "    # Residual stream width, I don't know of an easy way to infer it from the above config.\n",
    "    d_model = model.params[\"token_embed\"]['embeddings'].shape[1]\n",
    "\n",
    "    # Equivalent to length of vocab, WITHOUT BOS and PAD at the end because we never care about these outputs\n",
    "    # In practice, we always feed the logits into an argmax\n",
    "    d_vocab_out = model.params[\"token_embed\"]['embeddings'].shape[0] - 2\n",
    "\n",
    "    cfg = HookedTransformerConfig(\n",
    "        n_layers=n_layers,\n",
    "        d_model=d_model,\n",
    "        d_head=d_head,\n",
    "        n_ctx=n_ctx,\n",
    "        d_vocab=d_vocab,\n",
    "        d_vocab_out=d_vocab_out,\n",
    "        d_mlp=d_mlp,\n",
    "        n_heads=n_heads,\n",
    "        act_fn=act_fn,\n",
    "        attention_dir=attention_type,\n",
    "        normalization_type=normalization_type,\n",
    "    )\n",
    "    tl_model = HookedTransformer(cfg)\n",
    "\n",
    "\n",
    "    # %%\n",
    "    sd = {}\n",
    "    sd[\"pos_embed.W_pos\"] = model.params[\"pos_embed\"]['embeddings']\n",
    "    sd[\"embed.W_E\"] = model.params[\"token_embed\"]['embeddings']\n",
    "    # Equivalent to max_seq_len plus one, for the BOS\n",
    "\n",
    "    # The unembed is just a projection onto the first few elements of the residual stream, these store output tokens\n",
    "    # This is a NumPy array, the rest are Jax Arrays, but w/e it's fine.\n",
    "    sd[\"unembed.W_U\"] = np.eye(d_model, d_vocab_out)\n",
    "\n",
    "    for l in range(n_layers):\n",
    "        sd[f\"blocks.{l}.attn.W_K\"] = einops.rearrange(\n",
    "            model.params[f\"transformer/layer_{l}/attn/key\"][\"w\"],\n",
    "            \"d_model (n_heads d_head) -> n_heads d_model d_head\",\n",
    "            d_head = d_head,\n",
    "            n_heads = n_heads\n",
    "        )\n",
    "        sd[f\"blocks.{l}.attn.b_K\"] = einops.rearrange(\n",
    "            model.params[f\"transformer/layer_{l}/attn/key\"][\"b\"],\n",
    "            \"(n_heads d_head) -> n_heads d_head\",\n",
    "            d_head = d_head,\n",
    "            n_heads = n_heads\n",
    "        )\n",
    "        sd[f\"blocks.{l}.attn.W_Q\"] = einops.rearrange(\n",
    "            model.params[f\"transformer/layer_{l}/attn/query\"][\"w\"],\n",
    "            \"d_model (n_heads d_head) -> n_heads d_model d_head\",\n",
    "            d_head = d_head,\n",
    "            n_heads = n_heads\n",
    "        )\n",
    "        sd[f\"blocks.{l}.attn.b_Q\"] = einops.rearrange(\n",
    "            model.params[f\"transformer/layer_{l}/attn/query\"][\"b\"],\n",
    "            \"(n_heads d_head) -> n_heads d_head\",\n",
    "            d_head = d_head,\n",
    "            n_heads = n_heads\n",
    "        )\n",
    "        sd[f\"blocks.{l}.attn.W_V\"] = einops.rearrange(\n",
    "            model.params[f\"transformer/layer_{l}/attn/value\"][\"w\"],\n",
    "            \"d_model (n_heads d_head) -> n_heads d_model d_head\",\n",
    "            d_head = d_head,\n",
    "            n_heads = n_heads\n",
    "        )\n",
    "        sd[f\"blocks.{l}.attn.b_V\"] = einops.rearrange(\n",
    "            model.params[f\"transformer/layer_{l}/attn/value\"][\"b\"],\n",
    "            \"(n_heads d_head) -> n_heads d_head\",\n",
    "            d_head = d_head,\n",
    "            n_heads = n_heads\n",
    "        )\n",
    "        sd[f\"blocks.{l}.attn.W_O\"] = einops.rearrange(\n",
    "            model.params[f\"transformer/layer_{l}/attn/linear\"][\"w\"],\n",
    "            \"(n_heads d_head) d_model -> n_heads d_head d_model\",\n",
    "            d_head = d_head,\n",
    "            n_heads = n_heads\n",
    "        )\n",
    "        sd[f\"blocks.{l}.attn.b_O\"] = model.params[f\"transformer/layer_{l}/attn/linear\"][\"b\"]\n",
    "\n",
    "        sd[f\"blocks.{l}.mlp.W_in\"] = model.params[f\"transformer/layer_{l}/mlp/linear_1\"][\"w\"]\n",
    "        sd[f\"blocks.{l}.mlp.b_in\"] = model.params[f\"transformer/layer_{l}/mlp/linear_1\"][\"b\"]\n",
    "        sd[f\"blocks.{l}.mlp.W_out\"] = model.params[f\"transformer/layer_{l}/mlp/linear_2\"][\"w\"]\n",
    "        sd[f\"blocks.{l}.mlp.b_out\"] = model.params[f\"transformer/layer_{l}/mlp/linear_2\"][\"b\"]\n",
    "    print(sd.keys())\n",
    "\n",
    "\n",
    "    for k, v in sd.items():\n",
    "        # I cannot figure out a neater way to go from a Jax array to a numpy array lol\n",
    "        sd[k] = torch.tensor(np.array(v))\n",
    "\n",
    "    tl_model.load_state_dict(sd, strict=False)\n",
    "\n",
    "    return tl_model\n",
    "\n",
    "tl_model = tracr_to_tl(model)\n",
    "\n",
    "INPUT_ENCODER = model.input_encoder\n",
    "OUTPUT_ENCODER = model.output_encoder\n",
    "\n",
    "def create_model_input(input, input_encoder=INPUT_ENCODER):\n",
    "    encoding = input_encoder.encode(input)\n",
    "    return torch.tensor(encoding).unsqueeze(dim=0)\n",
    "\n",
    "def decode_model_output(logits, output_encoder=OUTPUT_ENCODER, bos_token=INPUT_ENCODER.bos_token):\n",
    "    max_output_indices = logits.squeeze(dim=0).argmax(dim=-1)\n",
    "    decoded_output = output_encoder.decode(max_output_indices.tolist())\n",
    "    decoded_output_with_bos = [bos_token] + decoded_output[1:]\n",
    "    return decoded_output_with_bos\n",
    "\n",
    "input = [bos, 1, 2, 3]\n",
    "out = model.apply(input)\n",
    "print(\"Original Decoding:\", out.decoded)\n",
    "\n",
    "input_tokens_tensor = create_model_input(input)\n",
    "logits = tl_model(input_tokens_tensor)\n",
    "decoded_output = decode_model_output(logits)\n",
    "print(\"TransformerLens Replicated Decoding:\", decoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6, 4, 1, 2, 1, 6],\n",
      "        [2, 1, 5, 6, 5, 6],\n",
      "        [6, 6, 1, 2, 6, 6],\n",
      "        ...,\n",
      "        [6, 2, 6, 6, 5, 6],\n",
      "        [4, 1, 2, 3, 3, 1],\n",
      "        [2, 5, 2, 4, 2, 1]])\n",
      "torch.Size([46656, 6])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import itertools\n",
    "\n",
    "def permutations_with_replacement_to_tensor(s):\n",
    "    # Convert the set to a list to allow indexed access\n",
    "    elements = list(s)\n",
    "    n = len(elements)\n",
    "    \n",
    "    # Generate all permutations with replacement using itertools.product\n",
    "    # This creates an iterator for all n-length combinations of the elements\n",
    "    all_permutations = itertools.product(elements, repeat=n)\n",
    "    \n",
    "    # Convert iterator to list of lists\n",
    "    permutations_list = [list(perm) for perm in all_permutations]\n",
    "    \n",
    "    # Convert list of lists to a PyTorch tensor\n",
    "    # Each permutation is a row in the tensor\n",
    "    permutations_tensor = torch.tensor(permutations_list)\n",
    "\n",
    "    # Shuffle tensor\n",
    "    perm_indices = torch.randperm(permutations_tensor.size(0))\n",
    "    permutations_tensor = permutations_tensor[perm_indices]\n",
    "    \n",
    "    return permutations_tensor\n",
    "\n",
    "# Example usage\n",
    "s = {1, 2, 3, 4, 5, 6}\n",
    "tensor = permutations_with_replacement_to_tensor(s)\n",
    "print(tensor)\n",
    "print(tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['attn_q_0', 'attn_k_0', 'attn_v_0', 'attn_q_3', 'attn_k_3', 'attn_v_3']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def groundtruth_tl_tracr_model(model):\n",
    "    circuit_components = []\n",
    "    \n",
    "    for i in range(model.cfg.n_layers):\n",
    "        # Check if W_Q is not all zero\n",
    "        if not torch.allclose(model.W_Q[i], torch.zeros_like(model.W_Q[i])):\n",
    "            circuit_components.append(f\"attn_q_{i}\")\n",
    "        # Check if W_K is not all zero\n",
    "        if not torch.allclose(model.W_K[i], torch.zeros_like(model.W_K[i])):\n",
    "            circuit_components.append(f\"attn_k_{i}\")\n",
    "        # Check if W_V is not all zero\n",
    "        if not torch.allclose(model.W_V[i], torch.zeros_like(model.W_V[i])):\n",
    "            circuit_components.append(f\"attn_v_{i}\")\n",
    "\n",
    "    return circuit_components\n",
    "\n",
    "ground_truth = groundtruth_tl_tracr_model(tl_model)\n",
    "ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([250, 12, 12])\n",
      "['attn_k_0', 'attn_q_0', 'attn_v_0', 'attn_k_1', 'attn_q_1', 'attn_v_1', 'attn_k_2', 'attn_q_2', 'attn_v_2', 'attn_k_3', 'attn_q_3', 'attn_v_3']\n"
     ]
    }
   ],
   "source": [
    "def resid_cache_from_tl_tracr_model(model, input_tokens_tensor):\n",
    "    _, cache = model.run_with_cache(input_tokens_tensor)\n",
    "    labels = []\n",
    "\n",
    "    # For each layer, get the K, Q and V tensors and MLP in and out\n",
    "    attn_hook_k = []\n",
    "    attn_hook_q = []\n",
    "    attn_hook_v = []\n",
    "    for i in range(model.cfg.n_layers):\n",
    "        hook_k = cache[f'blocks.{i}.attn.hook_k']\n",
    "        #hook_k = torch.nn.functional.pad(hook_k, (0, model.cfg.d_model - hook_k.shape[-1]))\n",
    "        attn_hook_k.append(hook_k.squeeze().mean(dim=1) + torch.randn_like(hook_k.squeeze().mean(dim=1)))\n",
    "        labels.append(f\"attn_k_{i}\")\n",
    "\n",
    "        hook_q = cache[f'blocks.{i}.attn.hook_q']\n",
    "        #hook_q = torch.nn.functional.pad(hook_q, (0, model.cfg.d_model - hook_q.shape[-1]))\n",
    "        attn_hook_q.append(hook_q.squeeze().mean(dim=1) + torch.randn_like(hook_q.squeeze().mean(dim=1)))\n",
    "        labels.append(f\"attn_q_{i}\")\n",
    "\n",
    "        hook_v = cache[f'blocks.{i}.attn.hook_v']\n",
    "        #hook_v = torch.nn.functional.pad(hook_v, (0, model.cfg.d_model - hook_v.shape[-1]))\n",
    "        attn_hook_v.append(hook_v.squeeze().mean(dim=1) + torch.randn_like(hook_v.squeeze().mean(dim=1)))\n",
    "        labels.append(f\"attn_v_{i}\")\n",
    "\n",
    "    # Stack all tensors together\n",
    "    attn_hook_k = torch.stack(attn_hook_k)\n",
    "    attn_hook_q = torch.stack(attn_hook_q)\n",
    "    attn_hook_v = torch.stack(attn_hook_v)\n",
    "\n",
    "    # Stack everything\n",
    "    final_resid = torch.cat([attn_hook_k, attn_hook_q, attn_hook_v], dim=0)\n",
    "    return einops.rearrange(final_resid, 'c n d -> n c d'), labels\n",
    "    \n",
    "head_resid, head_labels = resid_cache_from_tl_tracr_model(tl_model, tensor[:250, :])\n",
    "print(head_resid.shape)\n",
    "print(head_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeroing out attn_k_0\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='mps:0')\n",
      "Zeroing out attn_q_0\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='mps:0')\n",
      "Zeroing out attn_v_0\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='mps:0')\n",
      "Corrupting attn_k_1\n",
      "tensor([[ 0.9897,  0.7680,  0.2630,  ...,  0.0629,  1.0821,  1.5981],\n",
      "        [ 0.0575,  0.6795, -0.8268,  ...,  1.0445,  0.6082,  0.6372],\n",
      "        [ 0.8694, -0.9262, -0.2278,  ...,  1.1967, -0.1929,  3.5138],\n",
      "        ...,\n",
      "        [ 0.0444,  0.6761,  0.2234,  ...,  2.1529, -0.6515,  0.5802],\n",
      "        [ 0.6291, -0.9395, -2.1160,  ..., -1.2453,  0.0227, -1.5304],\n",
      "        [ 0.4096,  0.8379,  0.5918,  ..., -1.7227, -0.1265, -0.3312]],\n",
      "       device='mps:0')\n",
      "Corrupting attn_q_1\n",
      "tensor([[ 6.7380e+01,  6.8028e+01,  6.6484e+01,  ...,  1.4403e+00,\n",
      "          1.0130e-01,  2.5291e+00],\n",
      "        [ 6.7051e+01,  6.6952e+01,  6.5122e+01,  ..., -2.3404e-02,\n",
      "         -7.0044e-02, -5.4363e-02],\n",
      "        [ 3.6340e+01,  3.3023e+01,  3.5324e+01,  ..., -4.0159e-01,\n",
      "          1.5019e+00,  1.6203e+00],\n",
      "        ...,\n",
      "        [ 8.3091e+01,  7.9936e+01,  8.0996e+01,  ..., -2.2788e-01,\n",
      "         -8.2853e-01, -1.9580e+00],\n",
      "        [ 8.1901e+01,  8.4587e+01,  8.4512e+01,  ...,  2.8308e-01,\n",
      "          1.0214e+00, -5.8900e-01],\n",
      "        [ 6.8522e+01,  6.7207e+01,  6.9006e+01,  ..., -1.2735e+00,\n",
      "          4.5249e-01,  1.0583e+00]], device='mps:0')\n",
      "Corrupting attn_v_1\n",
      "tensor([[ 1.4420, -2.1324,  0.4414,  ...,  0.8198,  1.0941, -1.5123],\n",
      "        [-0.3648, -3.6927, -1.7624,  ..., -0.2989,  1.8001,  1.4481],\n",
      "        [-1.0153, -1.4859, -1.9413,  ..., -0.8586,  2.3419,  0.7788],\n",
      "        ...,\n",
      "        [-1.8584, -0.2203,  0.0039,  ...,  2.0793, -1.6261,  0.2263],\n",
      "        [ 1.9134, -0.1205,  0.7857,  ...,  1.8685, -1.3588,  1.2955],\n",
      "        [-2.0860, -1.6434, -1.6330,  ...,  0.2604,  0.4029,  0.4643]],\n",
      "       device='mps:0')\n",
      "Corrupting attn_k_2\n",
      "tensor([[ 0.1606,  1.1213, -0.7677,  ..., -1.4500,  0.9251, -0.2720],\n",
      "        [ 0.6461, -1.4300, -0.9885,  ..., -2.9874,  2.0694,  0.0529],\n",
      "        [ 0.4428,  1.1171, -1.4169,  ...,  2.6809, -2.3459, -0.4624],\n",
      "        ...,\n",
      "        [-0.2581,  1.1211,  2.6676,  ..., -0.8081,  0.0965, -0.7765],\n",
      "        [ 3.1263,  3.5658,  0.0080,  ..., -0.7617,  1.3190, -0.3904],\n",
      "        [-2.1898, -1.2844, -2.0982,  ..., -0.8335, -1.6285, -1.0405]],\n",
      "       device='mps:0')\n",
      "Corrupting attn_q_2\n",
      "tensor([[ 1.7670e+01,  1.7848e+01,  1.9119e+00,  ...,  1.5786e-01,\n",
      "         -1.5329e+00, -5.1532e-01],\n",
      "        [ 1.6556e+01,  1.7193e+01,  5.9497e-02,  ...,  1.2591e+00,\n",
      "          1.2460e+00,  3.5175e-02],\n",
      "        [ 9.8543e-01, -1.1569e+00, -1.4425e+00,  ...,  2.6993e+00,\n",
      "         -1.0863e+00, -6.6250e-01],\n",
      "        ...,\n",
      "        [-8.9637e-01,  1.6401e+01,  1.8042e+01,  ..., -4.8351e-01,\n",
      "         -1.9388e-02, -1.4476e-01],\n",
      "        [ 1.2721e+01,  6.3364e-01,  1.9946e+01,  ...,  8.6653e-01,\n",
      "         -6.7565e-01, -1.0051e+00],\n",
      "        [ 1.9509e+01,  1.7260e+00,  1.3093e+00,  ...,  1.4010e+00,\n",
      "          2.9077e+00, -3.7337e-01]], device='mps:0')\n",
      "Corrupting attn_v_2\n",
      "tensor([[ 0.2560,  3.6223,  3.1326,  ...,  2.7740,  0.2015, -0.7205],\n",
      "        [-0.1568, -2.3971, -1.7225,  ..., -2.9869, -1.2552, -1.7447],\n",
      "        [ 1.3663,  0.2788, -1.7605,  ...,  0.9913,  2.1875,  0.2801],\n",
      "        ...,\n",
      "        [ 0.3518,  0.8072, -0.6825,  ...,  0.7630, -1.3106,  0.2258],\n",
      "        [ 0.6595,  1.3215, -1.0973,  ..., -0.1174,  1.2568,  0.4623],\n",
      "        [ 0.9587,  1.5696, -0.7006,  ..., -1.4977,  3.0768, -0.2448]],\n",
      "       device='mps:0')\n",
      "Zeroing out attn_k_3\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='mps:0')\n",
      "Zeroing out attn_q_3\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='mps:0')\n",
      "Zeroing out attn_v_3\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='mps:0')\n",
      "torch.Size([250, 12, 12])\n"
     ]
    }
   ],
   "source": [
    "# Corrupted head resid is going through ground-truth circuit\n",
    "corrupted_head_resid = head_resid.clone() #torch.randn_like(head_resid)\n",
    "# If component is in ground-truth, set its resid to 0 everywhere, else add Gaussian noise\n",
    "corrupted_head_resid = head_resid.clone()\n",
    "for i, component in enumerate(head_labels):\n",
    "    if component in ground_truth:\n",
    "        print(f\"Zeroing out {component}\")\n",
    "        corrupted_head_resid[:, i] = 0\n",
    "        print(corrupted_head_resid[:, i])\n",
    "    else:\n",
    "        print(f\"Corrupting {component}\")\n",
    "        corrupted_head_resid[:, i] += torch.randn_like(corrupted_head_resid[:, i])\n",
    "        print(corrupted_head_resid[:, i])\n",
    "\n",
    "print(corrupted_head_resid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack them together and save\n",
    "resid_streams = torch.cat([head_resid, corrupted_head_resid], dim=0)\n",
    "path = \"../data/tracr-reverse/\"\n",
    "torch.save(resid_streams, path + \"resid_heads_mean.pt\")\n",
    "# Save the head labels\n",
    "torch.save(head_labels, path + \"labels_heads_mean.pt\")\n",
    "# Save the ground truth\n",
    "torch.save(ground_truth, path + \"ground_truth.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tracr-xproportion`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BOS', 4.4194817525719976e-16, 0.5, 0.3333333432674408, 0.25]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tracr.compiler.lib import make_frac_prevs\n",
    "\n",
    "model = compiling.compile_rasp_to_model(\n",
    "      make_frac_prevs(rasp.tokens == \"x\"),\n",
    "      vocab={\"w\", \"x\", \"y\", \"z\"},\n",
    "      max_seq_len=6,\n",
    "      compiler_bos=\"BOS\",\n",
    "      )\n",
    "\n",
    "out = model.apply([\"BOS\", \"w\", \"x\", \"y\", \"z\"])\n",
    "out.decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['pos_embed.W_pos', 'embed.W_E', 'unembed.W_U', 'blocks.0.attn.W_K', 'blocks.0.attn.b_K', 'blocks.0.attn.W_Q', 'blocks.0.attn.b_Q', 'blocks.0.attn.W_V', 'blocks.0.attn.b_V', 'blocks.0.attn.W_O', 'blocks.0.attn.b_O', 'blocks.0.mlp.W_in', 'blocks.0.mlp.b_in', 'blocks.0.mlp.W_out', 'blocks.0.mlp.b_out', 'blocks.1.attn.W_K', 'blocks.1.attn.b_K', 'blocks.1.attn.W_Q', 'blocks.1.attn.b_Q', 'blocks.1.attn.W_V', 'blocks.1.attn.b_V', 'blocks.1.attn.W_O', 'blocks.1.attn.b_O', 'blocks.1.mlp.W_in', 'blocks.1.mlp.b_in', 'blocks.1.mlp.W_out', 'blocks.1.mlp.b_out'])\n"
     ]
    }
   ],
   "source": [
    "tl_model = tracr_to_tl(model)\n",
    "\n",
    "INPUT_ENCODER = model.input_encoder\n",
    "OUTPUT_ENCODER = model.output_encoder\n",
    "\n",
    "def create_model_input(input, input_encoder=INPUT_ENCODER):\n",
    "    encoding = input_encoder.encode(input)\n",
    "    return torch.tensor(encoding).unsqueeze(dim=0)\n",
    "\n",
    "def decode_model_output(logits, output_encoder=OUTPUT_ENCODER, bos_token=INPUT_ENCODER.bos_token):\n",
    "    max_output_indices = logits.squeeze(dim=0).argmax(dim=-1)\n",
    "    decoded_output = output_encoder.decode(max_output_indices.tolist())\n",
    "    decoded_output_with_bos = [bos_token] + decoded_output[1:]\n",
    "    return decoded_output_with_bos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Decoding: ['BOS', 1.0, 0.5, 0.3333333432674408, 0.5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[4.2045e-08, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [1.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [5.0000e-01, 0.0000e+00, 1.0000e+00, 0.0000e+00],\n",
       "         [3.3333e-01, 0.0000e+00, 0.0000e+00, 1.0000e+00],\n",
       "         [5.0000e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00]]], device='mps:0',\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = [bos, \"x\", \"w\", \"w\", \"x\"]\n",
    "out = model.apply(input)\n",
    "print(\"Original Decoding:\", out.decoded)\n",
    "\n",
    "input_tokens_tensor = create_model_input(input)\n",
    "logits = tl_model(input_tokens_tensor)\n",
    "\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3, 0, 3, 1],\n",
      "        [0, 2, 1, 1],\n",
      "        [1, 0, 0, 1],\n",
      "        ...,\n",
      "        [2, 1, 0, 2],\n",
      "        [3, 2, 0, 1],\n",
      "        [0, 2, 3, 2]])\n",
      "torch.Size([256, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import itertools\n",
    "\n",
    "def permutations_with_string_to_tensor(input_string):\n",
    "    # Map each unique character to a unique integer\n",
    "    unique_chars = set(input_string)\n",
    "    char_to_index = {char: i for i, char in enumerate(unique_chars)}\n",
    "    \n",
    "    # Convert input_string to indices\n",
    "    indices = [char_to_index[char] for char in input_string]\n",
    "    n = len(indices)\n",
    "    \n",
    "    # Generate all permutations with replacement using itertools.product\n",
    "    all_permutations = itertools.product(indices, repeat=n)\n",
    "    \n",
    "    # Convert iterator to list of lists\n",
    "    permutations_list = [list(perm) for perm in all_permutations]\n",
    "    \n",
    "    # Convert list of lists to a PyTorch tensor of type long\n",
    "    permutations_tensor = torch.tensor(permutations_list, dtype=torch.long)\n",
    "    \n",
    "    # Shuffle tensor\n",
    "    perm_indices = torch.randperm(permutations_tensor.size(0))\n",
    "    permutations_tensor = permutations_tensor[perm_indices]\n",
    "    \n",
    "    return permutations_tensor\n",
    "\n",
    "# Example usage\n",
    "s = {\"w\", \"x\", \"y\", \"z\"}\n",
    "tensor = permutations_with_string_to_tensor(s)\n",
    "print(tensor)\n",
    "print(tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['attn_q_1', 'attn_k_1', 'attn_v_1']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth = groundtruth_tl_tracr_model(tl_model)\n",
    "ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([250, 6, 8])\n",
      "['attn_k_0', 'attn_q_0', 'attn_v_0', 'attn_k_1', 'attn_q_1', 'attn_v_1']\n"
     ]
    }
   ],
   "source": [
    "head_resid, head_labels = resid_cache_from_tl_tracr_model(tl_model, tensor[:250, :])\n",
    "print(head_resid.shape)\n",
    "print(head_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrupting attn_k_0\n",
      "tensor([[-0.4801, -1.2460, -0.9162,  ..., -3.5015, -0.8105, -0.6292],\n",
      "        [-0.7809,  0.5113, -1.2693,  ...,  1.1346,  0.7321, -0.5356],\n",
      "        [-1.6931, -1.8289,  0.5035,  ..., -2.7695,  0.0945, -0.5288],\n",
      "        ...,\n",
      "        [ 0.5398, -0.4671, -0.3707,  ..., -1.1552,  0.4300,  0.8700],\n",
      "        [-2.1709,  2.3685, -0.0539,  ..., -0.5963, -1.0306, -0.0327],\n",
      "        [ 0.5795,  3.8303,  1.3946,  ...,  1.9283, -1.0473,  0.5952]],\n",
      "       device='mps:0')\n",
      "Corrupting attn_q_0\n",
      "tensor([[-1.1757,  1.4945,  1.1378,  ..., -0.8040, -0.5693, -0.7115],\n",
      "        [ 0.2037,  2.7882, -0.2064,  ..., -0.0908,  0.6268,  2.5802],\n",
      "        [ 1.0321,  0.0652,  0.1803,  ..., -0.3469, -0.4476, -3.3302],\n",
      "        ...,\n",
      "        [ 1.6057,  3.2537, -1.4321,  ...,  0.4479,  2.0291,  0.1824],\n",
      "        [ 0.6302,  1.1082,  0.5714,  ..., -1.3209,  3.1089, -0.4411],\n",
      "        [-0.8440, -0.4489,  1.0039,  ...,  1.2577,  1.6996,  0.4194]],\n",
      "       device='mps:0')\n",
      "Corrupting attn_v_0\n",
      "tensor([[ 0.1370,  2.5469,  0.7680,  ...,  0.1990,  2.0558, -0.9431],\n",
      "        [ 0.5102,  1.5314,  1.7890,  ..., -1.0936, -1.8387,  1.0429],\n",
      "        [-0.3685, -0.7627,  0.6459,  ...,  0.9795, -1.1819, -1.3494],\n",
      "        ...,\n",
      "        [ 0.4189,  1.4034, -0.0515,  ...,  0.6700, -2.6112,  0.8305],\n",
      "        [ 0.8060, -0.0899,  0.1612,  ...,  1.5868,  0.4129, -1.3476],\n",
      "        [ 0.2359,  1.5215, -0.4318,  ...,  0.4663, -0.1470, -1.4024]],\n",
      "       device='mps:0')\n",
      "Zeroing out attn_k_1\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='mps:0')\n",
      "Zeroing out attn_q_1\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='mps:0')\n",
      "Zeroing out attn_v_1\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='mps:0')\n",
      "torch.Size([250, 6, 8])\n"
     ]
    }
   ],
   "source": [
    "# Corrupted head resid is going through ground-truth circuit\n",
    "# If component is in ground-truth, set its resid to 0 everywhere, else add Gaussian noise\n",
    "corrupted_head_resid = head_resid.clone()\n",
    "for i, component in enumerate(head_labels):\n",
    "    if component in ground_truth:\n",
    "        print(f\"Zeroing out {component}\")\n",
    "        corrupted_head_resid[:, i] = 0\n",
    "        print(corrupted_head_resid[:, i])\n",
    "    else:\n",
    "        print(f\"Corrupting {component}\")\n",
    "        corrupted_head_resid[:, i] += torch.randn_like(corrupted_head_resid[:, i])\n",
    "        print(corrupted_head_resid[:, i])\n",
    "\n",
    "print(corrupted_head_resid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack them together and save\n",
    "resid_streams = torch.cat([head_resid, corrupted_head_resid], dim=0)\n",
    "path = \"../data/tracr-fracprev/\"\n",
    "torch.save(resid_streams, path + \"resid_heads_mean.pt\")\n",
    "# Save the head labels\n",
    "torch.save(head_labels, path + \"labels_heads_mean.pt\")\n",
    "# Save the ground truth\n",
    "torch.save(ground_truth, path + \"ground_truth.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tracr-count`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Decoding: ['BOS', 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "from tracr.compiler.lib import make_sort\n",
    "import tracr.compiler.lib as lib\n",
    "\n",
    "vocab = {1, 2, 3, 4, 5, 6}\n",
    "max_seq_len = 6\n",
    "program = lib.make_sort(rasp.tokens, rasp.tokens, max_seq_len=max_seq_len, min_key=1)\n",
    "\n",
    "model = compiling.compile_rasp_to_model(\n",
    "    program,\n",
    "    vocab=vocab,\n",
    "    max_seq_len=max_seq_len,\n",
    "    compiler_bos=\"BOS\",\n",
    ")\n",
    "\n",
    "out = model.apply([\"BOS\", 3, 2, 1])\n",
    "print(\"Original Decoding:\", out.decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['pos_embed.W_pos', 'embed.W_E', 'unembed.W_U', 'blocks.0.attn.W_K', 'blocks.0.attn.b_K', 'blocks.0.attn.W_Q', 'blocks.0.attn.b_Q', 'blocks.0.attn.W_V', 'blocks.0.attn.b_V', 'blocks.0.attn.W_O', 'blocks.0.attn.b_O', 'blocks.0.mlp.W_in', 'blocks.0.mlp.b_in', 'blocks.0.mlp.W_out', 'blocks.0.mlp.b_out', 'blocks.1.attn.W_K', 'blocks.1.attn.b_K', 'blocks.1.attn.W_Q', 'blocks.1.attn.b_Q', 'blocks.1.attn.W_V', 'blocks.1.attn.b_V', 'blocks.1.attn.W_O', 'blocks.1.attn.b_O', 'blocks.1.mlp.W_in', 'blocks.1.mlp.b_in', 'blocks.1.mlp.W_out', 'blocks.1.mlp.b_out', 'blocks.2.attn.W_K', 'blocks.2.attn.b_K', 'blocks.2.attn.W_Q', 'blocks.2.attn.b_Q', 'blocks.2.attn.W_V', 'blocks.2.attn.b_V', 'blocks.2.attn.W_O', 'blocks.2.attn.b_O', 'blocks.2.mlp.W_in', 'blocks.2.mlp.b_in', 'blocks.2.mlp.W_out', 'blocks.2.mlp.b_out'])\n"
     ]
    }
   ],
   "source": [
    "tl_model = tracr_to_tl(model)\n",
    "\n",
    "INPUT_ENCODER = model.input_encoder\n",
    "OUTPUT_ENCODER = model.output_encoder\n",
    "\n",
    "def create_model_input(input, input_encoder=INPUT_ENCODER):\n",
    "    encoding = input_encoder.encode(input)\n",
    "    return torch.tensor(encoding).unsqueeze(dim=0)\n",
    "\n",
    "def decode_model_output(logits, output_encoder=OUTPUT_ENCODER, bos_token=INPUT_ENCODER.bos_token):\n",
    "    max_output_indices = logits.squeeze(dim=0).argmax(dim=-1)\n",
    "    decoded_output = output_encoder.decode(max_output_indices.tolist())\n",
    "    decoded_output_with_bos = [bos_token] + decoded_output[1:]\n",
    "    return decoded_output_with_bos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 3, 1, 5, 4, 5],\n",
      "        [2, 3, 5, 6, 1, 2],\n",
      "        [6, 6, 1, 4, 3, 6],\n",
      "        ...,\n",
      "        [3, 1, 3, 2, 3, 6],\n",
      "        [3, 1, 6, 2, 4, 1],\n",
      "        [5, 3, 6, 4, 5, 2]])\n",
      "torch.Size([46656, 6])\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "s = {1, 2, 3, 4, 5, 6}\n",
    "tensor = permutations_with_replacement_to_tensor(s)\n",
    "print(tensor)\n",
    "print(tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['attn_q_1', 'attn_k_1', 'attn_v_1', 'attn_q_2', 'attn_k_2', 'attn_v_2']"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth = groundtruth_tl_tracr_model(tl_model)\n",
    "ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([250, 9, 38])\n",
      "['attn_k_0', 'attn_q_0', 'attn_v_0', 'attn_k_1', 'attn_q_1', 'attn_v_1', 'attn_k_2', 'attn_q_2', 'attn_v_2']\n"
     ]
    }
   ],
   "source": [
    "head_resid, head_labels = resid_cache_from_tl_tracr_model(tl_model, tensor[:250, :])\n",
    "print(head_resid.shape)\n",
    "print(head_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrupting attn_k_0\n",
      "tensor([[ 2.3146, -1.5225,  0.4979,  ...,  0.3677, -1.4383, -1.5451],\n",
      "        [ 3.4963, -0.3185, -0.6330,  ..., -1.3146,  0.9055,  0.9480],\n",
      "        [ 0.7630,  0.3394, -0.1548,  ..., -2.7482,  0.9594,  1.7201],\n",
      "        ...,\n",
      "        [-0.0616,  1.0126,  0.2954,  ...,  2.9095, -1.6609, -0.0842],\n",
      "        [ 0.4575, -0.9681, -0.0613,  ...,  4.3568, -1.9714, -0.1920],\n",
      "        [ 1.4183,  1.2655, -1.9858,  ..., -0.1248,  1.5747,  0.1844]],\n",
      "       device='mps:0')\n",
      "Corrupting attn_q_0\n",
      "tensor([[-0.9063,  0.4985, -2.2621,  ...,  0.8974, -0.4606,  2.1218],\n",
      "        [-1.4335, -0.4144,  1.5515,  ..., -0.9970,  2.8793, -1.8183],\n",
      "        [-0.6547,  0.0983,  0.1909,  ...,  0.0919,  1.7877, -0.9679],\n",
      "        ...,\n",
      "        [-2.7284,  0.8806, -1.4835,  ..., -0.5953,  0.0109, -1.0188],\n",
      "        [-0.0322,  0.5039,  0.1045,  ...,  0.1217,  1.5475, -0.3798],\n",
      "        [-2.3106, -1.0567, -0.7819,  ..., -0.6577,  2.0268,  2.8858]],\n",
      "       device='mps:0')\n",
      "Corrupting attn_v_0\n",
      "tensor([[ 0.1504,  0.1643, -0.3153,  ...,  0.1862, -1.6125,  0.4394],\n",
      "        [ 2.7882,  0.9713, -1.8637,  ..., -2.1921,  2.0302, -1.8159],\n",
      "        [ 3.2861,  0.7990, -0.6424,  ...,  2.2269, -1.5905, -1.0066],\n",
      "        ...,\n",
      "        [-1.1619, -2.5276,  2.3420,  ...,  0.8273,  0.1968, -2.1465],\n",
      "        [-0.0814, -3.1825,  0.2850,  ..., -1.6235,  0.1725,  2.1798],\n",
      "        [ 1.2094, -0.2137,  0.1507,  ..., -0.3025, -0.7023,  2.1373]],\n",
      "       device='mps:0')\n",
      "Zeroing out attn_k_1\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='mps:0')\n",
      "Zeroing out attn_q_1\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='mps:0')\n",
      "Zeroing out attn_v_1\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='mps:0')\n",
      "Zeroing out attn_k_2\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='mps:0')\n",
      "Zeroing out attn_q_2\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='mps:0')\n",
      "Zeroing out attn_v_2\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='mps:0')\n",
      "torch.Size([250, 9, 38])\n"
     ]
    }
   ],
   "source": [
    "# Corrupted head resid is going through ground-truth circuit\n",
    "# If component is in ground-truth, set its resid to 0 everywhere, else add Gaussian noise\n",
    "corrupted_head_resid = head_resid.clone()\n",
    "for i, component in enumerate(head_labels):\n",
    "    if component in ground_truth:\n",
    "        print(f\"Zeroing out {component}\")\n",
    "        corrupted_head_resid[:, i] = 0\n",
    "        print(corrupted_head_resid[:, i])\n",
    "    else:\n",
    "        print(f\"Corrupting {component}\")\n",
    "        corrupted_head_resid[:, i] += torch.randn_like(corrupted_head_resid[:, i])\n",
    "        print(corrupted_head_resid[:, i])\n",
    "\n",
    "print(corrupted_head_resid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack them together and save\n",
    "resid_streams = torch.cat([head_resid, corrupted_head_resid], dim=0)\n",
    "path = \"../data/tracr-sort/\"\n",
    "torch.save(resid_streams, path + \"resid_heads_mean.pt\")\n",
    "# Save the head labels\n",
    "torch.save(head_labels, path + \"labels_heads_mean.pt\")\n",
    "# Save the ground truth\n",
    "torch.save(ground_truth, path + \"ground_truth.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tracr-sortfreq`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Decoding: ['BOS', 'a', 'a', 'a', 'b', 'b', 'c']\n"
     ]
    }
   ],
   "source": [
    "from tracr.compiler.lib import make_sort\n",
    "import tracr.compiler.lib as lib\n",
    "\n",
    "vocab = {'a', 'b', 'c', 'd', 'e', 'f'}\n",
    "max_seq_len = 6\n",
    "program = lib.make_sort_freq(max_seq_len=max_seq_len)\n",
    "\n",
    "model = compiling.compile_rasp_to_model(\n",
    "    program,\n",
    "    vocab=vocab,\n",
    "    max_seq_len=max_seq_len,\n",
    "    compiler_bos=\"BOS\",\n",
    ")\n",
    "\n",
    "out = model.apply([\"BOS\", 'a', 'a', 'b', 'a', 'b', 'c'])\n",
    "print(\"Original Decoding:\", out.decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['pos_embed.W_pos', 'embed.W_E', 'unembed.W_U', 'blocks.0.attn.W_K', 'blocks.0.attn.b_K', 'blocks.0.attn.W_Q', 'blocks.0.attn.b_Q', 'blocks.0.attn.W_V', 'blocks.0.attn.b_V', 'blocks.0.attn.W_O', 'blocks.0.attn.b_O', 'blocks.0.mlp.W_in', 'blocks.0.mlp.b_in', 'blocks.0.mlp.W_out', 'blocks.0.mlp.b_out', 'blocks.1.attn.W_K', 'blocks.1.attn.b_K', 'blocks.1.attn.W_Q', 'blocks.1.attn.b_Q', 'blocks.1.attn.W_V', 'blocks.1.attn.b_V', 'blocks.1.attn.W_O', 'blocks.1.attn.b_O', 'blocks.1.mlp.W_in', 'blocks.1.mlp.b_in', 'blocks.1.mlp.W_out', 'blocks.1.mlp.b_out', 'blocks.2.attn.W_K', 'blocks.2.attn.b_K', 'blocks.2.attn.W_Q', 'blocks.2.attn.b_Q', 'blocks.2.attn.W_V', 'blocks.2.attn.b_V', 'blocks.2.attn.W_O', 'blocks.2.attn.b_O', 'blocks.2.mlp.W_in', 'blocks.2.mlp.b_in', 'blocks.2.mlp.W_out', 'blocks.2.mlp.b_out', 'blocks.3.attn.W_K', 'blocks.3.attn.b_K', 'blocks.3.attn.W_Q', 'blocks.3.attn.b_Q', 'blocks.3.attn.W_V', 'blocks.3.attn.b_V', 'blocks.3.attn.W_O', 'blocks.3.attn.b_O', 'blocks.3.mlp.W_in', 'blocks.3.mlp.b_in', 'blocks.3.mlp.W_out', 'blocks.3.mlp.b_out', 'blocks.4.attn.W_K', 'blocks.4.attn.b_K', 'blocks.4.attn.W_Q', 'blocks.4.attn.b_Q', 'blocks.4.attn.W_V', 'blocks.4.attn.b_V', 'blocks.4.attn.W_O', 'blocks.4.attn.b_O', 'blocks.4.mlp.W_in', 'blocks.4.mlp.b_in', 'blocks.4.mlp.W_out', 'blocks.4.mlp.b_out'])\n"
     ]
    }
   ],
   "source": [
    "tl_model = tracr_to_tl(model)\n",
    "\n",
    "INPUT_ENCODER = model.input_encoder\n",
    "OUTPUT_ENCODER = model.output_encoder\n",
    "\n",
    "def create_model_input(input, input_encoder=INPUT_ENCODER):\n",
    "    encoding = input_encoder.encode(input)\n",
    "    return torch.tensor(encoding).unsqueeze(dim=0)\n",
    "\n",
    "def decode_model_output(logits, output_encoder=OUTPUT_ENCODER, bos_token=INPUT_ENCODER.bos_token):\n",
    "    max_output_indices = logits.squeeze(dim=0).argmax(dim=-1)\n",
    "    decoded_output = output_encoder.decode(max_output_indices.tolist())\n",
    "    decoded_output_with_bos = [bos_token] + decoded_output[1:]\n",
    "    return decoded_output_with_bos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5, 3, 1, 2, 4, 2],\n",
      "        [0, 2, 5, 0, 4, 4],\n",
      "        [1, 3, 2, 5, 5, 2],\n",
      "        ...,\n",
      "        [1, 0, 0, 4, 1, 1],\n",
      "        [3, 5, 0, 4, 5, 0],\n",
      "        [5, 2, 2, 5, 5, 0]])\n",
      "torch.Size([46656, 6])\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "s = {'a', 'b', 'c', 'd', 'e', 'f'}\n",
    "tensor = permutations_with_string_to_tensor(s)\n",
    "print(tensor)\n",
    "print(tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['attn_q_0',\n",
       " 'attn_k_0',\n",
       " 'attn_v_0',\n",
       " 'attn_q_3',\n",
       " 'attn_k_3',\n",
       " 'attn_v_3',\n",
       " 'attn_q_4',\n",
       " 'attn_k_4',\n",
       " 'attn_v_4']"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth = groundtruth_tl_tracr_model(tl_model)\n",
    "ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([250, 15, 44])\n",
      "['attn_k_0', 'attn_q_0', 'attn_v_0', 'attn_k_1', 'attn_q_1', 'attn_v_1', 'attn_k_2', 'attn_q_2', 'attn_v_2', 'attn_k_3', 'attn_q_3', 'attn_v_3', 'attn_k_4', 'attn_q_4', 'attn_v_4']\n"
     ]
    }
   ],
   "source": [
    "head_resid, head_labels = resid_cache_from_tl_tracr_model(tl_model, tensor[:250, :])\n",
    "print(head_resid.shape)\n",
    "print(head_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeroing out attn_k_0\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='mps:0')\n",
      "Zeroing out attn_q_0\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='mps:0')\n",
      "Zeroing out attn_v_0\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='mps:0')\n",
      "Corrupting attn_k_1\n",
      "tensor([[-0.1315, -0.2047,  1.4654,  ..., -0.7236, -1.1208,  1.7551],\n",
      "        [ 0.6224, -0.9399,  0.9719,  ...,  0.1138,  0.4534, -0.6088],\n",
      "        [ 1.3511, -0.6247,  1.8356,  ...,  0.9733, -1.8882, -1.8385],\n",
      "        ...,\n",
      "        [ 1.4753,  0.3152, -0.4670,  ..., -1.5762,  1.4088, -0.5497],\n",
      "        [ 0.6116, -1.6789, -2.0357,  ..., -0.3823, -0.2820, -0.9756],\n",
      "        [ 0.3779,  0.4754,  2.9107,  ...,  1.1422, -0.4076, -0.9105]],\n",
      "       device='mps:0')\n",
      "Corrupting attn_q_1\n",
      "tensor([[ 2.2863, -0.2258,  0.0579,  ...,  1.8336,  0.2653,  0.9440],\n",
      "        [-1.9835,  1.8459, -0.3412,  ...,  0.0180,  1.9773, -1.3939],\n",
      "        [ 1.7752,  1.6372,  2.1834,  ...,  0.4426, -1.1947,  1.9257],\n",
      "        ...,\n",
      "        [-0.7486,  3.8663, -0.2468,  ..., -3.2294, -0.9548, -1.1167],\n",
      "        [-2.4804, -3.0226,  0.6160,  ...,  2.0086,  2.9732, -0.7814],\n",
      "        [-0.3125, -3.3533, -0.0316,  ...,  0.1991,  2.0733, -0.6431]],\n",
      "       device='mps:0')\n",
      "Corrupting attn_v_1\n",
      "tensor([[ 9.8336e+01,  1.5555e+01,  3.4760e+01,  ...,  1.1349e+00,\n",
      "         -2.1685e-01, -2.4306e-02],\n",
      "        [ 1.0181e+02, -1.4822e+00,  1.4423e+01,  ...,  2.5601e+00,\n",
      "          1.7257e-01,  2.0806e+00],\n",
      "        [ 1.0160e+02,  1.7670e+01,  3.3422e+01,  ...,  7.9618e-01,\n",
      "          1.3116e+00, -1.5038e-01],\n",
      "        ...,\n",
      "        [ 1.0108e+02,  4.9967e+01,  1.6526e+01,  ...,  1.8682e+00,\n",
      "          2.0520e+00,  6.2901e-02],\n",
      "        [ 1.0152e+02,  3.2050e+01,  2.7292e-02,  ..., -1.1999e+00,\n",
      "          8.1136e-01, -2.7824e-01],\n",
      "        [ 1.0326e+02,  3.3334e+01,  3.3987e+01,  ...,  3.5755e-01,\n",
      "         -1.6367e+00, -2.2394e+00]], device='mps:0')\n",
      "Corrupting attn_k_2\n",
      "tensor([[ 0.6659, -0.7469,  0.1711,  ...,  0.8079,  0.3297,  1.2187],\n",
      "        [ 2.7437,  0.3318,  1.8016,  ...,  1.5259,  0.1279, -0.3315],\n",
      "        [-0.2214,  0.7973,  0.5102,  ...,  1.1912,  1.2444, -0.7863],\n",
      "        ...,\n",
      "        [-1.2804, -0.5391,  1.8333,  ..., -0.2780,  1.9240, -0.5167],\n",
      "        [-1.4491,  0.7158, -1.0993,  ...,  0.9359, -1.8171, -1.9848],\n",
      "        [-0.7080, -1.7522, -1.3613,  ..., -1.0949, -1.0983, -0.1394]],\n",
      "       device='mps:0')\n",
      "Corrupting attn_q_2\n",
      "tensor([[ 0.8845,  0.2042, -0.2357,  ...,  1.9098,  1.4493, -2.0290],\n",
      "        [-1.0607, -0.0266,  0.1156,  ...,  0.5254,  0.6038, -3.0425],\n",
      "        [ 0.0440, -0.2098, -1.1796,  ...,  3.0153,  1.1154, -1.4047],\n",
      "        ...,\n",
      "        [ 0.0117, -1.9529,  0.2250,  ..., -2.1383,  0.4895, -0.8728],\n",
      "        [-2.6333, -1.0541,  1.1357,  ...,  1.5336,  0.3812,  2.7364],\n",
      "        [-0.6791,  1.4970, -2.7978,  ..., -0.0341, -0.5842, -1.2659]],\n",
      "       device='mps:0')\n",
      "Corrupting attn_v_2\n",
      "tensor([[ 6.4624e+01,  4.9139e+01,  3.2956e+01,  ...,  4.4747e-01,\n",
      "          9.9315e+01,  3.0761e-01],\n",
      "        [ 6.8050e+01,  6.5293e+01,  6.5711e+01,  ...,  1.9185e+00,\n",
      "          9.8065e+01,  1.0717e+00],\n",
      "        [ 6.7157e+01,  5.0595e+01,  3.4580e+01,  ..., -1.0703e-01,\n",
      "          9.9384e+01, -2.8506e-02],\n",
      "        ...,\n",
      "        [ 6.6625e+01,  4.9331e+01,  3.4169e+01,  ...,  6.9039e-01,\n",
      "          1.0088e+02, -2.0222e+00],\n",
      "        [ 6.6134e+01,  6.8107e+01,  6.6039e+01,  ..., -7.7788e-01,\n",
      "          1.0043e+02,  1.1289e+00],\n",
      "        [ 8.2458e+01,  8.3129e+01,  8.2984e+01,  ..., -5.5748e+00,\n",
      "          1.0206e+02, -1.7528e+00]], device='mps:0')\n",
      "Zeroing out attn_k_3\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='mps:0')\n",
      "Zeroing out attn_q_3\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='mps:0')\n",
      "Zeroing out attn_v_3\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='mps:0')\n",
      "Zeroing out attn_k_4\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='mps:0')\n",
      "Zeroing out attn_q_4\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='mps:0')\n",
      "Zeroing out attn_v_4\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='mps:0')\n",
      "torch.Size([250, 15, 44])\n"
     ]
    }
   ],
   "source": [
    "# Corrupted head resid is going through ground-truth circuit\n",
    "corrupted_head_resid = head_resid.clone() #torch.randn_like(head_resid)\n",
    "# If component is in ground-truth, set its resid to 0 everywhere, else add Gaussian noise\n",
    "corrupted_head_resid = head_resid.clone()\n",
    "for i, component in enumerate(head_labels):\n",
    "    if component in ground_truth:\n",
    "        print(f\"Zeroing out {component}\")\n",
    "        corrupted_head_resid[:, i] = 0\n",
    "        print(corrupted_head_resid[:, i])\n",
    "    else:\n",
    "        print(f\"Corrupting {component}\")\n",
    "        corrupted_head_resid[:, i] += torch.randn_like(corrupted_head_resid[:, i])\n",
    "        print(corrupted_head_resid[:, i])\n",
    "\n",
    "print(corrupted_head_resid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack them together and save\n",
    "resid_streams = torch.cat([head_resid, corrupted_head_resid], dim=0)\n",
    "path = \"../data/tracr-sortfreq/\"\n",
    "torch.save(resid_streams, path + \"resid_heads_mean.pt\")\n",
    "# Save the head labels\n",
    "torch.save(head_labels, path + \"labels_heads_mean.pt\")\n",
    "# Save the ground truth\n",
    "torch.save(ground_truth, path + \"ground_truth.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
